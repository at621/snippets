{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e322809-b24f-472c-9703-86a8f23d391d",
   "metadata": {},
   "source": [
    "### Create documentation\n",
    "\n",
    "**PROMPT**  \n",
    "You are a subject matter expert on credit risk policies, regulatory compliance (e.g., Basel III),   \n",
    "and internal governance standards. I have a set of documents covering regulations, internal   \n",
    "guidelines, and reference materials. I also have a Table of Contents (ToC) that outlines how I   \n",
    "want the final policy to be structured. \n",
    "\n",
    "Using only the content retrieved from the provided documents and guided by the ToC, please   \n",
    "create each policy section. For each section, incorporate references to relevant regulatory   \n",
    "requirements, define any necessary terminology, and outline best practices for credit risk   \n",
    "assessment, approval workflows, monitoring, reporting, governance, and compliance. \n",
    "\n",
    "Keep the language concise, clear, and aligned with standard industry formats. Structure your   \n",
    "final output according to the headings in the ToC, ensuring a coherent, well-organized policy   \n",
    "that meets both regulatory and internal standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49962e85-f29d-4a82-a54f-ada46ff5a878",
   "metadata": {},
   "source": [
    "#### CreditRiskPolicyAgent Overview\n",
    "\n",
    "- **Purpose**: Automate creation and refinement of a Credit Risk Policy document  \n",
    "- **Components**:  \n",
    "  - **DocumentCreator**: Fetches context from Chroma and drafts each section  \n",
    "  - **DocumentEvaluator**: Reviews each draft, providing feedback for improvement  \n",
    "  - **FinalReviewer**: Conducts a final holistic assessment of the entire policy  \n",
    "\n",
    "#### Workflow\n",
    "\n",
    "1. **Draft**:  \n",
    "   The creator pulls relevant text from the Chroma database.  \n",
    "   It then produces an initial section draft.  \n",
    "\n",
    "2. **Evaluate**:  \n",
    "   The evaluator checks accuracy, clarity, and compliance.  \n",
    "   Feedback is generated for each section.  \n",
    "\n",
    "3. **Refine**:  \n",
    "   The creator refines the draft, based on evaluator feedback.  \n",
    "\n",
    "4. **Final Review**:  \n",
    "   Once all sections are ready, the FinalReviewer inspects the entire policy.  \n",
    "   It ensures coherence and makes any final improvements.  \n",
    "\n",
    "#### Key Methods\n",
    "\n",
    "- **draft_section(section_title)**  \n",
    "  Creates a preliminary version of a policy section.  \n",
    "\n",
    "- **evaluate_section(draft_text)**  \n",
    "  Reviews the text for correctness and suggests enhancements.  \n",
    "\n",
    "- **refine_with_feedback(draft_text, feedback)**  \n",
    "  Applies feedback to produce a better draft.  \n",
    "\n",
    "- **review_document(entire_document)**  \n",
    "  Performs the last check on the policy as a whole.  \n",
    "\n",
    "#### Usage\n",
    "\n",
    "1. **Initialize** the CreditRiskPolicyAgent with your API key and Chroma collection.  \n",
    "2. **Provide** a table of contents for the policy.  \n",
    "3. **Call** `build_policy`, which executes draft, evaluate, refine, and final review steps.  \n",
    "4. **Receive** a fully formed policy document, ready for adoption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5eaf87-3f14-498a-b250-2c28905893c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 07:02:42,777 - INFO - Initializing CreditRiskPolicyAgent...\n",
      "2025-01-24 07:02:44,560 - INFO - Initializing DocumentCreator...\n",
      "2025-01-24 07:02:44,560 - INFO - DocumentCreator initialized.\n",
      "2025-01-24 07:02:44,561 - INFO - Initializing DocumentEvaluator...\n",
      "2025-01-24 07:02:44,561 - INFO - DocumentEvaluator initialized.\n",
      "2025-01-24 07:02:44,562 - INFO - Initializing FinalReviewer...\n",
      "2025-01-24 07:02:44,562 - INFO - FinalReviewer initialized.\n",
      "2025-01-24 07:02:44,563 - INFO - CreditRiskPolicyAgent initialized.\n",
      "2025-01-24 07:02:44,564 - INFO - Starting to build final document...\n",
      "2025-01-24 07:02:44,565 - INFO - Starting policy building process...\n",
      "2025-01-24 07:02:44,565 - INFO - Processing section: 'Introduction and Scope'...\n",
      "2025-01-24 07:02:44,566 - INFO - Drafting section: 'Introduction and Scope'...\n",
      "2025-01-24 07:02:44,567 - INFO - Retrieving context for section: 'Introduction and Scope'...\n",
      "2025-01-24 07:02:44,979 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:02:44,983 - INFO - Context length for section: 'Introduction and Scope'`: 0 characters.\n",
      "2025-01-24 07:03:14,702 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:14,710 - INFO - LLM call for drafting section 'Introduction and Scope' completed.\n",
      "2025-01-24 07:03:14,710 - INFO - Drafting section 'Introduction and Scope'\n",
      "2025-01-24 07:03:14,711 - INFO - Token usage: 632 (Prompt: 79, Completion: 553, Cost: $0.0355)\n",
      "2025-01-24 07:03:14,712 - INFO - Section 'Introduction and Scope' drafted.\n",
      "2025-01-24 07:03:14,712 - INFO - Evaluating section...\n",
      "2025-01-24 07:03:22,592 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:22,597 - INFO - LLM call for section evaluation completed.\n",
      "2025-01-24 07:03:22,597 - INFO - Evaluation token usage: 859 (Prompt: 598, Completion: 261, Cost: $0.0336)\n",
      "2025-01-24 07:03:22,598 - INFO - Section evaluated, feedback generated.\n",
      "2025-01-24 07:03:22,599 - INFO - Refining section with feedback...\n",
      "2025-01-24 07:03:35,220 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:35,226 - INFO - LLM call for refining section with feedback completed.\n",
      "2025-01-24 07:03:35,226 - INFO - Refinement token usage: 1438 (Prompt: 850, Completion: 588, Cost: $0.0608)\n",
      "2025-01-24 07:03:35,226 - INFO - Section refined with feedback.\n",
      "2025-01-24 07:03:35,227 - INFO - Section 'Introduction and Scope' processing complete.\n",
      "2025-01-24 07:03:35,228 - INFO - Processing section: 'Definitions and Terminology'...\n",
      "2025-01-24 07:03:35,228 - INFO - Drafting section: 'Definitions and Terminology'...\n",
      "2025-01-24 07:03:35,229 - INFO - Retrieving context for section: 'Definitions and Terminology'...\n",
      "2025-01-24 07:03:35,462 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:35,466 - INFO - Context length for section: 'Definitions and Terminology'`: 0 characters.\n",
      "2025-01-24 07:03:48,185 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:48,196 - INFO - LLM call for drafting section 'Definitions and Terminology' completed.\n",
      "2025-01-24 07:03:48,196 - INFO - Drafting section 'Definitions and Terminology'\n",
      "2025-01-24 07:03:48,197 - INFO - Token usage: 452 (Prompt: 74, Completion: 378, Cost: $0.0249)\n",
      "2025-01-24 07:03:48,198 - INFO - Section 'Definitions and Terminology' drafted.\n",
      "2025-01-24 07:03:48,199 - INFO - Evaluating section...\n",
      "2025-01-24 07:03:56,523 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:03:56,529 - INFO - LLM call for section evaluation completed.\n",
      "2025-01-24 07:03:56,529 - INFO - Evaluation token usage: 771 (Prompt: 423, Completion: 348, Cost: $0.0336)\n",
      "2025-01-24 07:03:56,530 - INFO - Section evaluated, feedback generated.\n",
      "2025-01-24 07:03:56,531 - INFO - Refining section with feedback...\n",
      "2025-01-24 07:04:11,130 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:04:11,135 - INFO - LLM call for refining section with feedback completed.\n",
      "2025-01-24 07:04:11,136 - INFO - Refinement token usage: 1282 (Prompt: 762, Completion: 520, Cost: $0.0541)\n",
      "2025-01-24 07:04:11,136 - INFO - Section refined with feedback.\n",
      "2025-01-24 07:04:11,137 - INFO - Section 'Definitions and Terminology' processing complete.\n",
      "2025-01-24 07:04:11,137 - INFO - Processing section: 'Governance and Compliance'...\n",
      "2025-01-24 07:04:11,138 - INFO - Drafting section: 'Governance and Compliance'...\n",
      "2025-01-24 07:04:11,139 - INFO - Retrieving context for section: 'Governance and Compliance'...\n",
      "2025-01-24 07:04:11,698 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:04:11,703 - INFO - Context length for section: 'Governance and Compliance'`: 0 characters.\n",
      "2025-01-24 07:04:26,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:04:26,406 - INFO - LLM call for drafting section 'Governance and Compliance' completed.\n",
      "2025-01-24 07:04:26,406 - INFO - Drafting section 'Governance and Compliance'\n",
      "2025-01-24 07:04:26,406 - INFO - Token usage: 573 (Prompt: 74, Completion: 499, Cost: $0.0322)\n",
      "2025-01-24 07:04:26,408 - INFO - Section 'Governance and Compliance' drafted.\n",
      "2025-01-24 07:04:26,408 - INFO - Evaluating section...\n",
      "2025-01-24 07:04:35,363 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:04:35,368 - INFO - LLM call for section evaluation completed.\n",
      "2025-01-24 07:04:35,369 - INFO - Evaluation token usage: 815 (Prompt: 544, Completion: 271, Cost: $0.0326)\n",
      "2025-01-24 07:04:35,369 - INFO - Section evaluated, feedback generated.\n",
      "2025-01-24 07:04:35,370 - INFO - Refining section with feedback...\n",
      "2025-01-24 07:05:00,466 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:05:00,470 - INFO - LLM call for refining section with feedback completed.\n",
      "2025-01-24 07:05:00,470 - INFO - Refinement token usage: 1341 (Prompt: 806, Completion: 535, Cost: $0.0563)\n",
      "2025-01-24 07:05:00,471 - INFO - Section refined with feedback.\n",
      "2025-01-24 07:05:00,471 - INFO - Section 'Governance and Compliance' processing complete.\n",
      "2025-01-24 07:05:00,472 - INFO - Conducting final document review...\n",
      "2025-01-24 07:05:20,517 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-01-24 07:05:20,537 - INFO - LLM call for final document review completed.\n",
      "2025-01-24 07:05:20,538 - INFO - Final review token usage: 2559 (Prompt: 1706, Completion: 853, Cost: $0.1024)\n",
      "2025-01-24 07:05:20,538 - INFO - Final document review completed.\n",
      "2025-01-24 07:05:20,539 - INFO - Policy building process completed.\n",
      "2025-01-24 07:05:20,539 - INFO - Total document creation cost: $0.4658\n",
      "2025-01-24 07:05:20,540 - INFO - Total tokens used for document creation: 10722\n",
      "2025-01-24 07:05:20,540 - INFO - Final document build complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DocumentCreator:\n",
    "    \"\"\"\n",
    "    Creates and refines policy sections.\n",
    "    Pulls context from Chroma by passing heading, overall doc, and section desc as the query.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, retriever):\n",
    "        logging.info(\"Initializing DocumentCreator...\")\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        logging.info(\"DocumentCreator initialized.\")\n",
    "\n",
    "    def retrieve_context(self, overall_desc, section_title, section_desc):\n",
    "        logging.info(f\"Retrieving context for section: '{section_title}'...\")\n",
    "        query_text = (\n",
    "            f\"Overall Document: {overall_desc}\\n\"\n",
    "            f\"Section Title: {section_title}\\n\"\n",
    "            f\"Section Description: {section_desc}\"\n",
    "        )\n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.get_relevant_documents(query_text)\n",
    "        context = \"\\n\".join([d.page_content for d in docs])\n",
    "        logging.info(f\"Context length for section: '{section_title}'`: {len(context)} characters.\")\n",
    "        return context\n",
    "\n",
    "    def draft_section(self, overall_desc, section_title, section_desc):\n",
    "        logging.info(f\"Drafting section: '{section_title}'...\")\n",
    "        # Create a chat prompt with system and user instructions\n",
    "        system_template = (\n",
    "            \"You are drafting a Credit Risk Policy section. \"\n",
    "            \"Incorporate best practices and references.\"\n",
    "        )\n",
    "        user_template = (\n",
    "            \"Section Title: {section_title}\\n\"\n",
    "            \"Overall Description: {overall_desc}\\n\"\n",
    "            \"Section Description: {section_desc}\\n\"\n",
    "            \"Relevant Context:\\n{context}\\n\\n\"\n",
    "            \"Draft the policy section.\"\n",
    "        )\n",
    "\n",
    "        system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        human_msg = HumanMessagePromptTemplate.from_template(user_template)\n",
    "\n",
    "        # Retrieve any relevant context from Chroma\n",
    "        context = self.retrieve_context(overall_desc, section_title, section_desc)\n",
    "\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "        formatted_prompt = chat_prompt.format_messages(\n",
    "            section_title=section_title,\n",
    "            overall_desc=overall_desc,\n",
    "            section_desc=section_desc,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            logging.info(f\"LLM call for drafting section '{section_title}' completed.\")\n",
    "            logging.info(f\"Drafting section '{section_title}'\")\n",
    "            logging.info(f\"Token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "            draft_cost = cb.total_cost\n",
    "            draft_tokens = cb.total_tokens\n",
    "\n",
    "        logging.info(f\"Section '{section_title}' drafted.\")\n",
    "        return response.content, draft_cost, draft_tokens # Return cost and tokens\n",
    "\n",
    "    def refine_with_feedback(self, draft, feedback):\n",
    "        logging.info(f\"Refining section with feedback...\")\n",
    "        system_template = (\n",
    "            \"You are refining a policy section draft based on reviewer feedback.\"\n",
    "        )\n",
    "        user_template = (\n",
    "            \"Original Draft:\\n{draft}\\n\\n\"\n",
    "            \"Reviewer Feedback:\\n{feedback}\\n\\n\"\n",
    "            \"Refine the draft, addressing all feedback.\"\n",
    "        )\n",
    "\n",
    "        system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        human_msg = HumanMessagePromptTemplate.from_template(user_template)\n",
    "\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "        formatted_prompt = chat_prompt.format_messages(draft=draft, feedback=feedback)\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            logging.info(f\"LLM call for refining section with feedback completed.\")\n",
    "            logging.info(f\"Refinement token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "            refine_cost = cb.total_cost\n",
    "            refine_tokens = cb.total_tokens\n",
    "\n",
    "        logging.info(f\"Section refined with feedback.\")\n",
    "        return response.content, refine_cost, refine_tokens # Return cost and tokens\n",
    "\n",
    "class DocumentEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates individual sections for correctness, clarity, and compliance.\n",
    "    Provides feedback for improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm):\n",
    "        logging.info(\"Initializing DocumentEvaluator...\")\n",
    "        self.llm = llm\n",
    "        logging.info(\"DocumentEvaluator initialized.\")\n",
    "\n",
    "    def evaluate_section(self, draft_section):\n",
    "        logging.info(\"Evaluating section...\")\n",
    "        system_template = \"You are an independent reviewer of a policy draft.\"\n",
    "        user_template = (\n",
    "            \"Draft Section:\\n{draft_section}\\n\\n\"\n",
    "            \"1) Evaluate correctness, clarity, and compliance.\\n\"\n",
    "            \"2) Suggest improvements.\\n\"\n",
    "            \"3) Provide concise feedback.\"\n",
    "        )\n",
    "\n",
    "        system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        human_msg = HumanMessagePromptTemplate.from_template(user_template)\n",
    "\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "        formatted_prompt = chat_prompt.format_messages(draft_section=draft_section)\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            logging.info(\"LLM call for section evaluation completed.\")\n",
    "            logging.info(f\"Evaluation token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "            eval_cost = cb.total_cost\n",
    "            eval_tokens = cb.total_tokens\n",
    "\n",
    "        logging.info(\"Section evaluated, feedback generated.\")\n",
    "        return response.content, eval_cost, eval_tokens # Return cost and tokens\n",
    "\n",
    "class FinalReviewer:\n",
    "    \"\"\"\n",
    "    Conducts a holistic review of the entire assembled policy.\n",
    "    Ensures coherence, consistency, and completeness.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm):\n",
    "        logging.info(\"Initializing FinalReviewer...\")\n",
    "        self.llm = llm\n",
    "        logging.info(\"FinalReviewer initialized.\")\n",
    "\n",
    "    def review_document(self, full_document):\n",
    "        logging.info(\"Conducting final document review...\")\n",
    "        system_template = (\n",
    "            \"You are a senior reviewer conducting a final review \"\n",
    "            \"of the entire policy.\"\n",
    "        )\n",
    "        user_template = (\n",
    "            \"Below is the entire policy:\\n\\n\"\n",
    "            \"{full_document}\\n\\n\"\n",
    "            \"1) Check consistency and completeness.\\n\"\n",
    "            \"2) Suggest improvements.\\n\"\n",
    "            \"3) Provide the final revised text.\"\n",
    "        )\n",
    "\n",
    "        system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "        human_msg = HumanMessagePromptTemplate.from_template(user_template)\n",
    "\n",
    "        chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "        formatted_prompt = chat_prompt.format_messages(full_document=full_document)\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            logging.info(\"LLM call for final document review completed.\")\n",
    "            logging.info(f\"Final review token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "            final_review_cost = cb.total_cost\n",
    "            final_review_tokens = cb.total_tokens\n",
    "\n",
    "        logging.info(\"Final document review completed.\")\n",
    "        return response.content, final_review_cost, final_review_tokens # Return cost and tokens\n",
    "\n",
    "class CreditRiskPolicyAgent:\n",
    "    \"\"\"\n",
    "    Orchestrates creation, section-level evaluation, refinement,\n",
    "    and a final holistic review.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key, chroma_collection):\n",
    "        logging.info(\"Initializing CreditRiskPolicyAgent...\")\n",
    "        embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "        store = Chroma(collection_name=chroma_collection, embedding_function=embeddings)\n",
    "\n",
    "        # ChatOpenAI is the newer recommended approach for OpenAI chat models\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-4\", api_key=api_key)\n",
    "\n",
    "        self.creator = DocumentCreator(self.llm, store.as_retriever())\n",
    "        self.evaluator = DocumentEvaluator(self.llm)\n",
    "        self.final_reviewer = FinalReviewer(self.llm)\n",
    "        logging.info(\"CreditRiskPolicyAgent initialized.\")\n",
    "\n",
    "    def build_policy(self, overall_desc, toc_dict):\n",
    "        logging.info(\"Starting policy building process...\")\n",
    "        refined_sections = {}\n",
    "        total_tokens_policy = 0 # Initialize total tokens\n",
    "        total_cost_policy = 0 # Initialize total cost\n",
    "\n",
    "        for key, s_info in toc_dict.items():\n",
    "            s_title = s_info[\"title\"]\n",
    "            s_desc = s_info[\"description\"]\n",
    "            logging.info(f\"Processing section: '{s_title}'...\")\n",
    "\n",
    "            # 1) Draft\n",
    "            draft, draft_cost, draft_tokens = self.creator.draft_section(overall_desc, s_title, s_desc)\n",
    "            total_cost_policy += draft_cost # Accumulate cost\n",
    "            total_tokens_policy += draft_tokens # Accumulate tokens\n",
    "\n",
    "            # 2) Evaluate\n",
    "            feedback, eval_cost, eval_tokens = self.evaluator.evaluate_section(draft)\n",
    "            total_cost_policy += eval_cost # Accumulate cost\n",
    "            total_tokens_policy += eval_tokens # Accumulate tokens\n",
    "\n",
    "            # 3) Refine\n",
    "            refined, refine_cost, refine_tokens = self.creator.refine_with_feedback(draft, feedback)\n",
    "            refined_sections[s_title] = refined\n",
    "            total_cost_policy += refine_cost # Accumulate cost\n",
    "            total_tokens_policy += refine_tokens # Accumulate tokens\n",
    "            logging.info(f\"Section '{s_title}' processing complete.\")\n",
    "\n",
    "        # Combine all sections for final review\n",
    "        entire_doc = \"\"\n",
    "        for s_title, text in refined_sections.items():\n",
    "            entire_doc += f\"{s_title}\\n\\n{text}\\n\\n\"\n",
    "\n",
    "        final_policy_output, final_review_cost, final_review_tokens = self.final_reviewer.review_document(entire_doc)\n",
    "        total_cost_policy += final_review_cost # Accumulate cost\n",
    "        total_tokens_policy += final_review_tokens # Accumulate tokens\n",
    "\n",
    "        final_policy = final_policy_output\n",
    "        logging.info(\"Policy building process completed.\")\n",
    "        logging.info(f\"Total document creation cost: ${total_cost_policy:.4f}\") # Log total cost\n",
    "        logging.info(f\"Total tokens used for document creation: {total_tokens_policy}\") # Log total tokens\n",
    "        return final_policy\n",
    "\n",
    "def main():\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    chroma_collection = \"risk_universe\"\n",
    "\n",
    "    overall_doc_description = (\n",
    "        \"This document sets out the company's Credit Risk Policy, \"\n",
    "        \"aligned with Basel III and internal guidelines.\"\n",
    "    )\n",
    "\n",
    "    toc = {\n",
    "        \"section_1\": {\n",
    "            \"title\": \"Introduction and Scope\",\n",
    "            \"description\": \"High-level overview, disclaimers, and scope\"\n",
    "        },\n",
    "        \"section_2\": {\n",
    "            \"title\": \"Definitions and Terminology\",\n",
    "            \"description\": \"Define all key terms\"\n",
    "        },\n",
    "        \"section_3\": {\n",
    "            \"title\": \"Governance and Compliance\",\n",
    "            \"description\": \"Governance structure and compliance needs\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    agent = CreditRiskPolicyAgent(api_key, chroma_collection)\n",
    "    logging.info(\"Starting to build final document...\")\n",
    "    final_document = agent.build_policy(overall_doc_description, toc)\n",
    "    logging.info(\"Final document build complete.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
