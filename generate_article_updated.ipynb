{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85cb85e-d4bb-4e30-9dbd-4aea333ad553",
   "metadata": {},
   "source": [
    "## Generate documents\n",
    "\n",
    "This Jupyter cell contains Python code designed to automatically generate a book in LaTeX format.  It leverages several key technologies to streamline the process:\n",
    "\n",
    "*   **OpenAI's language models (like o1):** To generate the actual content of each book section based on a defined outline and relevant background information.\n",
    "*   **Pandas:** To efficiently manage and load background data, which is expected to be pre-processed and saved in a pickle file. This background data contains text and pre-calculated embeddings for similarity searches.\n",
    "*   **Pickle:** To load the background data quickly from a `.pkl` file, preserving the data structure and embeddings.\n",
    "*   **LaTeX:** To format the generated book content into a professional, high-quality PDF document.\n",
    "\n",
    "**Here's a high-level overview of what the code does:**\n",
    "\n",
    "1.  **Loads Book Outline:** Reads a JSON file (`book_outline.json`) that defines the structure of your book (sections, titles, goals, and required background for each section).\n",
    "2.  **Loads Background Data:** Loads a pre-processed Pandas DataFrame from a pickle file (`regulations_with_embeddings.pkl`). This DataFrame should contain background text and their corresponding embeddings.\n",
    "3.  **Iterates Through Book Sections:**  Loops through each section defined in the book outline.\n",
    "4.  **Finds Relevant Background Text:** For each section, it uses cosine similarity to find the most relevant background text from the loaded DataFrame based on the \"required background\" description in the outline.\n",
    "5.  **Generates Section Content with OpenAI:**  Uses OpenAI's API to generate the text content for each section, providing the section title, goal, and the most similar background text as context to the language model.\n",
    "6.  **Formats Content in LaTeX:**  Structures the generated text into LaTeX sections, including proper LaTeX preamble and postamble for a complete document.  It also includes basic escaping of LaTeX special characters in titles and preamble.\n",
    "7.  **Saves LaTeX File:**  Saves the complete LaTeX code to a `.tex` file (`generated_book.tex`).\n",
    "8.  **Compiles LaTeX to PDF (Optional):**  Attempts to automatically compile the generated `.tex` file into a PDF document using `pdflatex`.\n",
    "\n",
    "This code provides a framework for automated book generation, and you can customize the outline, background data, prompts, and LaTeX formatting to create your own unique book.  Run the cell to start the book generation process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10ac4ae-b158-4054-bf1e-27d4156d356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openai\n",
    "import textwrap\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import subprocess\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import logging\n",
    "from typing import Any, Union, List, Dict\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b3db3-9aac-4382-9391-4cbad8b9c9e0",
   "metadata": {},
   "source": [
    "### A. Parametrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6eecf0-04ec-43b5-99b5-fd3fe14ace25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "JSON_OUTLINE_FILE = \"outline_v3.json\"  # Path to your JSON outline file\n",
    "GENERAL_KB = \"regulations_with_embeddings.pkl\"  # Path to your pickle file with background data\n",
    "VALIDATION_KB = 'validation_kb.pkl'\n",
    "TEXT_COLUMN_NAME = \"body_of_the_text\"  # Column with text content\n",
    "EMBEDDING_COLUMN_NAME = \"combined_text_embedding\"  # Column with pre-calculated embeddings\n",
    "OPENAI_MODEL = \"o1-preview\"  # \"gpt-4o\"  # Your preferred OpenAI model\n",
    "LATEX_OUTPUT_FILE = f\"validation_book_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.tex\"\n",
    "TABLE_INPUT_FILE = \"validation_table.tex\"\n",
    "\n",
    "# Global variable to track the total cost for the whole run\n",
    "TOTAL_COST = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf85ed9-9b21-4755-9f75-fed9df205ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"table_of_contents\": [\n",
    "#     {\n",
    "#       \"id\": \"1\",\n",
    "#       \"level\": 1,\n",
    "#       \"heading\": \"Introduction to Validation\",\n",
    "#       \"goal\": \"Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters.\",\n",
    "#       \"expected_length\": \"2 pages\",\n",
    "#       \"required_background\": \"Readers should be familiar with basic financial concepts such as loans, defaults, and interest rates, and have a general understanding of risk management principles.\"\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"1.1\",\n",
    "#       \"level\": 2,\n",
    "#       \"heading\": \"The role of Credit Risk Models\",\n",
    "#       \"goal\": \"Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology.\",\n",
    "#       \"expected_length\": \"1 page\",\n",
    "#       \"required_background\": \"A rudimentary knowledge of probability, statistics, and how banks estimate default risk is helpful.\"\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"1.2\",\n",
    "#       \"level\": 2,\n",
    "#       \"heading\": \"Assessment of validation tests\",\n",
    "#       \"goal\": \"Create a table with main validation tests and related thresholds. Explain ho this table should be used.\",\n",
    "#       \"expected_length\": \"1 page\",\n",
    "#       \"required_background\": \"Basic understanding of model performance metrics and statistical testing.\",\n",
    "#       \"details\": \"The table should have the following headers: Statistic, Green, Amber, Red. All headers should be bold. The table rows should be as follows: AUC (Area Under the ROC Curve) with Green: >= 0.75, Amber: 0.65 - 0.75, Red: < 0.65; Kolmogorov-Smirnov (KS) Statistic with Green: >= 0.40, Amber: 0.30 - 0.40, Red: < 0.30; Gini Coefficient with Green: >= 0.50, Amber: 0.30 - 0.50, Red: < 0.30; Brier Score with Green: <= 0.10, Amber: 0.10 - 0.20, Red: > 0.20; Hosmer-Lemeshow Statistic (p-value) with Green: > 0.05, Amber: 0.01 - 0.05, Red: <= 0.01; Accuracy with Green: >= 0.80, Amber: 0.70 - 0.80, Red: < 0.70; Precision (for default class) with Green: >= 0.60, Amber: 0.40 - 0.60, Red: < 0.40; Recall (for default class) with Green: >= 0.70, Amber: 0.50 - 0.70, Red: < 0.50; F1-Score (for default class) with Green: >= 0.65, Amber: 0.45 - 0.65, Red: < 0.45; Somers' D with Green: >= 0.50, Amber: 0.30 - 0.50, Red: < 0.30.\"\n",
    "#     }\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa23ee-ce15-4582-ae15-a61c4b844bb1",
   "metadata": {},
   "source": [
    "### B. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b3422d-53c6-478e-acc9-691d5a5d9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def load_json_outline(json_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads the book outline from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            outline = json.load(f)\n",
    "        logging.info(f\"Book outline loaded from {json_file}\")\n",
    "        return outline\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading JSON outline: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def import_latex_table(filename):\n",
    "    \"\"\"Imports the LaTeX table from a separate file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as infile:\n",
    "            table_content = infile.read()\n",
    "        return table_content\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Table file '{filename}' not found.\")\n",
    "        return \"\"  # Return an empty string so the program can continue\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading table file '{filename}': {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "\n",
    "def load_dataframe_from_pickle(pickle_filepath: str) -> Any:\n",
    "    \"\"\"Loads a DataFrame from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(pickle_filepath, \"rb\") as f:\n",
    "            loaded_df = pickle.load(f)\n",
    "        logging.info(f\"DataFrame loaded from pickle file: {pickle_filepath}\")\n",
    "        return loaded_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading DataFrame pickle: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_pandas_dataframe(pkl_file: str) -> Any:\n",
    "    \"\"\"\n",
    "    Loads a pandas DataFrame from a pickle file and converts embedding strings to numpy arrays.\n",
    "    Use this if your embeddings are stored as strings.\n",
    "    \"\"\"\n",
    "    df = load_dataframe_from_pickle(pkl_file)\n",
    "    try:\n",
    "        df[EMBEDDING_COLUMN_NAME] = df[EMBEDDING_COLUMN_NAME].apply(\n",
    "            lambda x: np.array(json.loads(x)) if isinstance(x, str) else x\n",
    "        )\n",
    "        logging.info(\"Embeddings converted to numpy arrays (if necessary).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting embeddings: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list:\n",
    "    \"\"\"Generates an embedding for the given text using OpenAI.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        response = openai.embeddings.create(input=[text], model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def find_similar_background_text(\n",
    "    df: pd.DataFrame,\n",
    "    background_description: str,\n",
    "    text_column: str,\n",
    "    embedding_column: str,\n",
    "    top_n: int = 5,\n",
    "    show_scores: bool = True\n",
    ") -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Finds the `top_n` most similar texts in `df` to the `background_description` \n",
    "    using cosine similarity. If `top_n`=1 (the default), returns a single string. \n",
    "    Otherwise, returns a list of the top results.\n",
    "    \n",
    "    :param df: DataFrame containing your text and embeddings\n",
    "    :param background_description: The text you want to compare to\n",
    "    :param text_column: Name of the column in df that has the text\n",
    "    :param embedding_column: Name of the column in df that has the embeddings\n",
    "    :param top_n: How many top results to return\n",
    "    :param show_scores: Whether to print the text and similarity score for each of the top results\n",
    "    :return: A single string if top_n == 1, otherwise a list of strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compute the embedding for the background description\n",
    "        description_embedding = np.array(get_embedding(background_description)).reshape(1, -1)\n",
    "        \n",
    "        # Make sure we have a numpy array of all embeddings\n",
    "        background_embeddings = np.vstack(df[embedding_column].to_numpy())  # shape: (num_rows, embedding_dim)\n",
    "\n",
    "        # Compute similarity: shape => (1, num_rows)\n",
    "        similarities = cosine_similarity(description_embedding, background_embeddings).flatten()\n",
    "        \n",
    "        # Get indices of top_n results, sorted by descending similarity\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "\n",
    "        # Build the top results\n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            text_val = df.iloc[idx][text_column]\n",
    "            score_val = similarities[idx]\n",
    "            top_results.append(text_val)\n",
    "            \n",
    "            # Optionally show each text with its similarity score\n",
    "            if show_scores:\n",
    "                print(f\"Similarity: {score_val:.4f} | Background: {background_description} | Text: {text_val}\")\n",
    "        \n",
    "        # Return a single string if top_n = 1, else return a list\n",
    "        if top_n == 1:\n",
    "            return top_results[0] if top_results else \"\"\n",
    "        else:\n",
    "            return top_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error finding similar background text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eaf254-dea5-4baf-ba76-e92894f4263f",
   "metadata": {},
   "source": [
    "### C. Content functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b073b2bc-075b-4fab-9f73-90f417ce2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_text(title: str, \n",
    "                          goal: str,\n",
    "                          level: int,\n",
    "                          background: str,\n",
    "                          references: str,\n",
    "                          model: str = OPENAI_MODEL,\n",
    "                          use_langchain: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Generates LaTeX-formatted text for either a section or a subsection.\n",
    "    \n",
    "    Parameters:\n",
    "      - title: The title of the section/subsection.\n",
    "      - goal: The goal of the section/subsection.\n",
    "      - background_text: Background content to guide the writing.\n",
    "      - level: Either \"section\" or \"subsection\". This will modify the prompt.\n",
    "      - model: The OpenAI model to use.\n",
    "      - use_langchain: If True, use LangChain's ChatOpenAI; otherwise, use openai.chat.completions.create.\n",
    "    \"\"\"\n",
    "    global TOTAL_COST\n",
    "    \n",
    "    header = f\"\"\"\n",
    "        You are a helpful AI assistant specialized in writing technical books about regulatory compliance and model validation in finance.\n",
    "\n",
    "        Level {level} heading | Heading: {title}\n",
    "\n",
    "        Goal of this book part of the book: {goal}\n",
    "\n",
    "        Background information to consider when writing:\n",
    "        { references }\n",
    "        { background }\n",
    "    \"\"\"\n",
    "    \n",
    "    instructions = f\"\"\"\n",
    "        ---\n",
    "        Write the content for the {title} above, keeping in mind the goal and background information.\n",
    "        Under no circumstance change the title label and NEVER create any additional Latex sections, subsections or sub-subsections.\n",
    "        Format the output as LaTeX, suitable for inclusion in a LaTeX document.\n",
    "        Level 1 heading stands for section, level 2 heading stands for subsection, level 3 stands for sub-subsection - add it to Latex. \n",
    "        Please use standard LaTeX commands for formatting (e.g., \\\\textbf{{important text}}, \\\\textit{{emphasized text}}).\n",
    "        If you need to include lists, use LaTeX list environments like \\\\begin{{itemize}} ... \\\\end{{itemize}} or \\\\begin{{enumerate}} ... \\\\end{{enumerate}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    instructions += \"For mathematical formulas, use inline math mode $...$ or display math mode \\\\begin{{equation}} ... \\\\end{{equation}}.\\n\"\n",
    "    instructions += \"Do not use mathematical formulas.\\n\\n\"\n",
    "    instructions += \"----\\n\"\n",
    "    instructions += \"When writing Python code, **format the output as valid Python code enclosed in ```python code blocks.**\\n\"\n",
    "    instructions += \"Include comments to explain the code where necessary.\\n\"\n",
    "    instructions += \"Focus on clarity, correctness, and efficiency of the Python code.\\n\"\n",
    "    instructions += \"Do not include any explanations outside of the code block.\\n\"\n",
    "\n",
    "    prompt = header + instructions\n",
    "    \n",
    "    # Remove any unwanted indentation from the multi-line string.\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    try:\n",
    "        if use_langchain:\n",
    "            llm = ChatOpenAI(model_name=model, temperature=1.0)\n",
    "            with get_openai_callback() as cb:\n",
    "                response = llm.invoke(prompt)\n",
    "                content = response.content.strip()\n",
    "                logging.info(\"LLM call for content generation completed.\")\n",
    "                logging.info(f\"Generation token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, \"\n",
    "                             f\"Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "\n",
    "                 # Accumulate the cost from this call\n",
    "                TOTAL_COST += cb.total_cost\n",
    "                \n",
    "            return content\n",
    "        else:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in LaTeX output for technical books.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                # temperature=0.7,\n",
    "                max_completion_tokens=1700\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating text for Level {level} '{title}': {e}\")\n",
    "        return f\"**Error generating content for this Level {level}. Please check logs.**\"\n",
    "\n",
    "\n",
    "def create_latex_section(section_text: str) -> str:\n",
    "    \"\"\"Formats a section with a LaTeX \\\\section header.\"\"\"\n",
    "    return f\"{section_text}\"\n",
    "\n",
    "\n",
    "def create_latex_subsection(subsection_text: str) -> str:\n",
    "    \"\"\"Formats a subsection with a LaTeX \\\\subsection header.\"\"\"\n",
    "    return f\"{subsection_text}\"\n",
    "\n",
    "\n",
    "def create_latex_preamble(title: str, author: str, header_text: str = 'Do you trust your credit risk models?') -> str:\n",
    "    \"\"\"\n",
    "    Creates the LaTeX preamble for the document, including a custom title page.\n",
    "    \"\"\"\n",
    "    preamble = f\"\"\"\n",
    "\\\\documentclass[12pt,a4paper]{{article}}\n",
    "\n",
    "\\\\usepackage[utf8]{{inputenc}}\n",
    "\\\\usepackage[T1]{{fontenc}}\n",
    "\\\\usepackage{{lmodern}}\n",
    "\\\\usepackage[margin=1in]{{geometry}}\n",
    "\\\\usepackage{{setspace}}\n",
    "\\\\usepackage{{titlesec}}\n",
    "\\\\usepackage{{etoolbox}}\n",
    "\\\\usepackage{{fancyhdr}}\n",
    "\\\\usepackage{{graphicx}}\n",
    "\\\\usepackage{{amsmath}}\n",
    "\\\\usepackage{{listings}} % For code listings\n",
    "\\\\usepackage[table]{{xcolor}}\n",
    "\\\\usepackage{{booktabs}}\n",
    "\\\\usepackage{{array}}\n",
    "\\\\usepackage{{caption}}\n",
    "\\\\usepackage{{ragged2e}}  % For \\RaggedRight in table cells\n",
    "\n",
    "\\\\lstset{{\n",
    "  basicstyle=\\\\ttfamily\\\\footnotesize,\n",
    "  breaklines=true,\n",
    "  showstringspaces=false\n",
    "}}\n",
    "\n",
    "% Ensure each \\\\section begins on a new page\n",
    "\\\\preto\\\\section{{\\\\clearpage}}\n",
    "\n",
    "% Format for section and subsection titles\n",
    "\\\\titleformat{{\\\\section}}{{\\\\large\\\\bfseries}}{{\\\\thesection}}{{1em}}{{}}\n",
    "\\\\titleformat{{\\\\subsection}}{{\\\\normalsize\\\\bfseries}}{{\\\\thesubsection}}{{1em}}{{}}\n",
    "\n",
    "\\\\setlength{{\\\\parindent}}{{10pt}}\n",
    "\\\\setlength{{\\\\parskip}}{{0.5\\\\baselineskip}}\n",
    "\\\\setlength{{\\\\headheight}}{{14.5pt}}\n",
    "\n",
    "\\\\pagestyle{{fancy}}\n",
    "\\\\fancyhf{{}}\n",
    "\\\\fancyhead[C]{{{header_text}}}\n",
    "\\\\fancyfoot[C]{{\\\\thepage}}\n",
    "\\\\renewcommand{{\\\\headrulewidth}}{{0pt}}\n",
    "\n",
    "\\\\begin{{document}}\n",
    "\\\\pagenumbering{{gobble}}\n",
    "\n",
    "% --- Custom Title Page ---\n",
    "\\\\begin{{titlepage}}\n",
    "    \\\\begin{{center}}\n",
    "        \\\\vspace*{{3cm}}\n",
    "        \n",
    "        {{\\\\Huge \\\\textbf{{{title}}}}}\\\\\\\\[0.8em]\n",
    "        \n",
    "        {{\\\\Large \\\\textit{{Review and application of key validation tests}}}}\\\\\\\\[2.5cm]\n",
    "        \n",
    "        {{\\\\large \\\\textbf{{{author}}}}}\\\\\\\\[0.5cm]\n",
    "        \n",
    "        \\\\vfill\n",
    "        {{\\\\large \\\\today}}\n",
    "    \\\\end{{center}}\n",
    "    \\\\thispagestyle{{empty}}\n",
    "\\\\end{{titlepage}}\n",
    "\n",
    "\\\\thispagestyle{{empty}}\n",
    "\\\\tableofcontents\n",
    "\\\\thispagestyle{{empty}} % No page number on ToC\n",
    "\\\\clearpage\n",
    "\\\\pagenumbering{{arabic}}\n",
    "\\\\setcounter{{page}}{{1}}\n",
    "\"\"\"\n",
    "    return preamble\n",
    "\n",
    "def create_latex_postamble() -> str:\n",
    "    \"\"\"Creates the LaTeX postamble for the document.\"\"\"\n",
    "    return \"\\n\\\\end{document}\\n\"\n",
    "\n",
    "\n",
    "def convert_markdown_code_blocks_to_lstlisting(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts Markdown code blocks (```python ... ```) into LaTeX lstlisting environments.\n",
    "    This helps prevent errors from raw backticks in the LaTeX document.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"```python\\s*(.*?)\\s*```\", re.DOTALL)\n",
    "    def repl(match):\n",
    "        code_content = match.group(1)\n",
    "        return \"\\\\begin{lstlisting}[language=Python]\\n\" + code_content + \"\\n\\\\end{lstlisting}\"\n",
    "    return pattern.sub(repl, text)\n",
    "\n",
    "\n",
    "def compile_latex_to_pdf(tex_filename):\n",
    "    \"\"\"\n",
    "    Compiles a .tex file to PDF using pdflatex (requires LaTeX installed).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run pdflatex twice to ensure references are updated if needed\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        print(\"PDF successfully generated.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during LaTeX compilation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130d573-949f-4f7a-9be9-748fd95339c9",
   "metadata": {},
   "source": [
    "### D. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9692a1d-d196-41ff-ab58-e36efae3b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 07:37:43,021 - INFO - Book outline loaded from outline_v3.json\n",
      "2025-02-13 07:37:43,031 - INFO - DataFrame loaded from pickle file: validation_kb.pkl\n",
      "2025-02-13 07:37:43,032 - INFO - Embeddings converted to numpy arrays (if necessary).\n",
      "2025-02-13 07:37:44,301 - INFO - DataFrame loaded from pickle file: regulations_with_embeddings.pkl\n",
      "2025-02-13 07:37:44,305 - INFO - Embeddings converted to numpy arrays (if necessary).\n",
      "2025-02-13 07:37:44,313 - INFO - Generating book content section by section...\n",
      "2025-02-13 07:37:44,314 - INFO - Processing section 1: Introduction to Validation\n",
      "2025-02-13 07:37:44,314 - INFO - Finding similar background for section intro...\n",
      "2025-02-13 07:37:45,193 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:37:46,264 - INFO - Generating text with OpenAI for section intro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8606 | Background: Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters. | Text: Section 1 recalls as an introduction the specificities of the validation in the context of the prudential framework, and in particular in terms of corporate governance and structural independence from the CRCU;\n",
      "Similarity: 0.8595 | Background: Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters. | Text: A sound validation function is crucial to ensuring the reliability of internal models and their ability to accurately compute capital requirements. It is the responsibility of the credit institution to ensure that its internal models are fully compliant with all regulatory requirements.\n",
      "Similarity: 0.8584 | Background: Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters. | Text: Credit institutions are required to develop validation processes to validate their internal models. 8 While all validation processes must comply with the same regulatory requirements, the ability to perform comparisons across models and institutions remains limited.\n",
      "Similarity: 0.8532 | Background: Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters. | Text: The sections detailing the various validation tools are all structured in the same way. First, the objective of the analysis is described, followed by a description of the tool. Credit institutions are then provided with detailed guidance on implementing the tool and the scope of its application. Lastly, institutions are given details of how to report the results of these tests and analyses.\n",
      "Similarity: 0.8515 | Background: Highlight why validation is essential for financial institutions and lay the groundwork for subsequent validation chapters. | Text: 28. Purpose  of  this  section .  This  section  recalls  the  main  dimensions  on  which  the  validation function is expected to have an opinion, without listing the exact validation tasks and analyses to be conducted to form such opinions. In any case, as mentioned in paragraph [25], this section does not prevent institutions from developing additional tools and methods. This is because, as stated in paragraph [26], the validation framework to be defined by each institution (e.g. processes, tests or frequencies) is expected to be tailored to the specificities of a given type of exposures or rating system. 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 07:38:15,092 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:38:15,103 - INFO - LLM call for content generation completed.\n",
      "2025-02-13 07:38:15,103 - INFO - Generation token usage: 3585 (Prompt: 703, Completion: 2882, Cost: $0.1835)\n",
      "2025-02-13 07:38:15,104 - INFO - Processing section 1.1: The role of Credit Risk Models\n",
      "2025-02-13 07:38:15,105 - INFO - Finding similar background for section intro...\n",
      "2025-02-13 07:38:15,686 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:38:16,756 - INFO - Generating text with OpenAI for section intro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8914 | Background: Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology. | Text: With regard to ELBE, in 29% of cases there was a dedicated model in place, while in 44% of cases ELBE was set as equal to the specific credit risk adjustments for the exposure. Of the cases where a dedicated model was in place, 62% based their expected loss estimation on the LGD performing model. Of the cases with a standalone model, the majority used empirical evidence based on internal data in the ELBE estimation.\n",
      "Similarity: 0.8895 | Background: Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology. | Text: 11 For the purposes of this document, the term “model” is defined using the definition set out in Section 2.4 of the EBA Guidelines on PD estimation, LGD estimation and the treatment of defaulted exposures (EBA/GL/2017/16; published on 20 November 2017). Specifically, a PD model is defined as “all data and methods used as part of a rating system within the meaning of Article 142(1) point (1) of Regulation (EU) No 575/2013, which relate to the differentiation and quantification of own estimates of PD and which are used to assess the default risk for each obligor or exposure covered by that model”. Likewise, an LGD model relates to “LGD […] used to assess the level of loss in the case of default for each facility covered by that model”. An illustrative depiction of rating systems, models and calibration segments can be found in (<>)Figure 1.\n",
      "Similarity: 0.8874 | Background: Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology. | Text: The review considered the risk parameters for internal credit risk models, namely the PD, LGD and CCF. The detailed results presented in this section of the report focus on the PD and LGD parameters, as these were the parameters most frequently assessed in the LDP investigations. The TRIM investigations also included a review of data management practices applied to the credit risk models under review, as well as an assessment of the quality of current and historical data used for IRB modelling purposes.\n",
      "Similarity: 0.8868 | Background: Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology. | Text: While different practices were observed regarding the risk drivers considered in the model, most institutions used contract characteristics (including type of collateral and type of products) in their LGD models. Obligor characteristics and, in the case of retail exposures, internal behavioural information (i.e. information on the delinquency of the obligor) were the other types of risk driver most commonly used (in around 35% and 20% of cases, respectively).\n",
      "Similarity: 0.8863 | Background: Introduce the main types of credit risk models (PD, LGD, EAD, ELBE) and explain their role in credit risk management. Provide a concise overview of the various credit risk model types, their typical use cases, and key differences in data and methodology. | Text: (<>)Figure 2 provides an illustrative overview of a generic model landscape. Individual models, each of which comprises ranking methods and calibration segments, may fall within the scope of more than one rating system. This practice may be more common for LGD and CCF models than for PD models. In the same vein, ELBE and LGD in-default models need not necessarily have the same range of application. That is reflected in the structure of this document, which deals with the assessment of such models in separate sections (see Sections (<>)2.7 and (<>)2.8 respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 07:40:13,699 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:40:13,701 - INFO - LLM call for content generation completed.\n",
      "2025-02-13 07:40:13,702 - INFO - Generation token usage: 4331 (Prompt: 1009, Completion: 3322, Cost: $0.2145)\n",
      "2025-02-13 07:40:13,702 - INFO - Processing section 1.2: Assessment of validation tests\n",
      "2025-02-13 07:40:13,703 - INFO - Finding similar background for section intro...\n",
      "2025-02-13 07:40:14,456 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:40:15,535 - INFO - Generating text with OpenAI for section intro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8182 | Background: Create a table with main validation tests and related thresholds. Explain ho this table should be used. | Text: Quantitative thresholds (see paragraph (<>)77(c)) should be set up for at least the following tests:\n",
      "Similarity: 0.8152 | Background: Create a table with main validation tests and related thresholds. Explain ho this table should be used. | Text: In particular for tests where no thresholds are applied, a consistent qualitative assessment of the results should be performed and documented. In the event of a negative qualitative assessment, adequate measures or actions should be triggered.\n",
      "Similarity: 0.8143 | Background: Create a table with main validation tests and related thresholds. Explain ho this table should be used. | Text: The content of the validation process should include quantitative analyses, which in turn should include thresholds. If such thresholds are breached, further investigation should be initiated and, if necessary, adequate measures or actions should be triggered.\n",
      "Similarity: 0.8045 | Background: Create a table with main validation tests and related thresholds. Explain ho this table should be used. | Text: As a good practice, the inclusion of a comparison between the latest results of the validation and the ones observed in the previous years as well as the highlighting of the previously identified deficiencies, along with their severity, and a description of how they have been addressed.\n",
      "Similarity: 0.8036 | Background: Create a table with main validation tests and related thresholds. Explain ho this table should be used. | Text: 28. Purpose  of  this  section .  This  section  recalls  the  main  dimensions  on  which  the  validation function is expected to have an opinion, without listing the exact validation tasks and analyses to be conducted to form such opinions. In any case, as mentioned in paragraph [25], this section does not prevent institutions from developing additional tools and methods. This is because, as stated in paragraph [26], the validation framework to be defined by each institution (e.g. processes, tests or frequencies) is expected to be tailored to the specificities of a given type of exposures or rating system. 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 07:41:41,713 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 07:41:41,715 - INFO - LLM call for content generation completed.\n",
      "2025-02-13 07:41:41,716 - INFO - Generation token usage: 4894 (Prompt: 1129, Completion: 3765, Cost: $0.2428)\n",
      "2025-02-13 07:41:41,717 - INFO - Saved LaTeX output to validation_book_20250213_073742.tex\n",
      "2025-02-13 07:41:41,718 - INFO - Book generation complete!\n",
      "2025-02-13 07:41:41,718 - INFO - Total cost of the run: $0.64\n",
      "2025-02-13 07:41:41,719 - INFO - Now you can compile 'validation_book_20250213_073742.tex' with LaTeX (e.g., pdflatex).\n"
     ]
    }
   ],
   "source": [
    "# --- Main Function (Modified for Flat Structure) ---\n",
    "def main():\n",
    "    \"\"\"Main function to generate the book content.\"\"\"\n",
    "    global TOTAL_COST  # Make sure to update this where you incur costs\n",
    "\n",
    "    # Load the book outline\n",
    "    book_outline = load_json_outline(JSON_OUTLINE_FILE)\n",
    "\n",
    "    # Load the background DataFrame.\n",
    "    try:\n",
    "        validation_df = load_pandas_dataframe(VALIDATION_KB)\n",
    "        general_df = load_pandas_dataframe(GENERAL_KB)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Something went wrong: {e}\")\n",
    "\n",
    "    # Merge with another knowledge base\n",
    "    cols = ['source', 'body_of_the_text', 'word_count', 'combined_text_embedding']\n",
    "    background_df = pd.concat([validation_df[cols], general_df[cols]], axis=0, ignore_index=True)\n",
    "\n",
    "    # Filter out too short or too long paragraphs\n",
    "    idx_1 = background_df['word_count'] > 10\n",
    "    idx_2 = background_df['word_count'] < 1000\n",
    "    background_df = background_df[idx_1 & idx_2]\n",
    "\n",
    "    # Create the LaTeX preamble\n",
    "    latex_content = create_latex_preamble(\n",
    "        title=\"Do you trust your risk models?\",\n",
    "        author=\"Collaboration between Human and AI\"\n",
    "    )\n",
    "    logging.info(\"Generating book content section by section...\")\n",
    "\n",
    "    # contents = book_outline[\"table_of_contents\"][:6]\n",
    "    contents = book_outline[\"table_of_contents\"]\n",
    "\n",
    "    for section_data in contents:\n",
    "        level = section_data[\"level\"]\n",
    "        heading = section_data[\"heading\"]\n",
    "        goal = section_data[\"goal\"]\n",
    "        background = section_data[\"required_background\"]\n",
    "        section_number = section_data[\"id\"]\n",
    "\n",
    "        if level == 0:\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing section {section_number}: {heading}\")\n",
    "        logging.info(\"Finding similar background for section intro...\")\n",
    "\n",
    "        references = find_similar_background_text(\n",
    "            background_df, goal, TEXT_COLUMN_NAME, EMBEDDING_COLUMN_NAME\n",
    "        )\n",
    "\n",
    "        logging.info(\"Generating text with OpenAI for section intro...\")\n",
    "\n",
    "        generated_section_text = generate_content_text(\n",
    "            title=heading,\n",
    "            goal=goal,\n",
    "            level=level,\n",
    "            background=background,\n",
    "            references=references,\n",
    "            model=OPENAI_MODEL,\n",
    "            use_langchain=True  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # Convert any Markdown code blocks to lstlisting environments\n",
    "        generated_section_text = convert_markdown_code_blocks_to_lstlisting(generated_section_text)\n",
    "        latex_content += create_latex_section(generated_section_text)\n",
    "\n",
    "    # Import the Validation Table\n",
    "    latex_content += \"\\\\clearpage\"\n",
    "    table_latex = import_latex_table(TABLE_INPUT_FILE)\n",
    "    latex_content += table_latex\n",
    "\n",
    "    latex_content += create_latex_postamble()\n",
    "\n",
    "    # Save the LaTeX output to a file\n",
    "    try:\n",
    "        with open(LATEX_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(latex_content)\n",
    "        logging.info(f\"Saved LaTeX output to {LATEX_OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving LaTeX file: {e}\")\n",
    "\n",
    "    logging.info(\"Book generation complete!\")\n",
    "    logging.info(f\"Total cost of the run: ${TOTAL_COST:.2f}\")\n",
    "    logging.info(f\"Now you can compile '{LATEX_OUTPUT_FILE}' with LaTeX (e.g., pdflatex).\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ce238-29a2-4cb7-96d1-575f4982853d",
   "metadata": {},
   "source": [
    "### E. Convert to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c301cc-6f0a-4ff1-88b7-e8b8cb6f527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdflatex -output-directory=C:/projects/generate_documents/latex_materials generated_book_v19.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17aacab-7ed3-4c0e-9c44-871bb58b60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF successfully generated and copied to the current directory.\n"
     ]
    }
   ],
   "source": [
    "def compile_latex_to_pdf(tex_filename, temp_dir):\n",
    "    \"\"\"\n",
    "    Compiles a .tex file to PDF using pdflatex in an existing temporary directory.\n",
    "    \n",
    "    This function copies all tex-related files from the current directory into\n",
    "    temp_dir, performs the LaTeX compilation there, and then copies the generated\n",
    "    PDF back to the original directory.\n",
    "    \n",
    "    Parameters:\n",
    "        tex_filename (str): The name of the .tex file to compile.\n",
    "        temp_dir (str): Path to an existing temporary directory.\n",
    "    \"\"\"\n",
    "    # Save the current working directory.\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Ensure the temporary directory exists.\n",
    "    if not os.path.isdir(temp_dir):\n",
    "        raise ValueError(f\"Temporary directory {temp_dir} does not exist.\")\n",
    "\n",
    "    try:\n",
    "        # Define file extensions relevant to the LaTeX project.\n",
    "        extensions = ['tex', 'bib', 'sty', 'cls', 'bst', 'png', 'jpg', 'jpeg', 'pdf', 'eps']\n",
    "        \n",
    "        # Copy each file with the specified extensions from the current directory to temp_dir.\n",
    "        for ext in extensions:\n",
    "            pattern = os.path.join(current_dir, f'*.{ext}')\n",
    "            for filepath in glob.glob(pattern):\n",
    "                shutil.copy(filepath, temp_dir)\n",
    "        \n",
    "        # Change the working directory to the temporary directory.\n",
    "        os.chdir(temp_dir)\n",
    "        \n",
    "        # Run pdflatex twice to ensure proper reference resolution.\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        \n",
    "        # Determine the output PDF's name.\n",
    "        pdf_filename = os.path.splitext(tex_filename)[0] + '.pdf'\n",
    "        \n",
    "        # Copy the PDF back to the original directory if it exists.\n",
    "        if os.path.exists(pdf_filename):\n",
    "            shutil.copy(pdf_filename, current_dir)\n",
    "            print(\"PDF successfully generated and copied to the current directory.\")\n",
    "        else:\n",
    "            print(\"PDF not found after compilation.\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during LaTeX compilation: {e}\")\n",
    "\n",
    "compile_latex_to_pdf(LATEX_OUTPUT_FILE, \"latex_materials\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
