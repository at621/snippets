{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae484e6-c52e-42d3-902b-55163b2b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d35bc1-49ab-42bf-9deb-7ce8b3675898",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "##### Maximize IV value of the bins\n",
    "\n",
    "IV = ∑((Good% - Bad%) * ln(Good% / Bad%)), where Good% and Bad% are the proportions of non-default and default observations, respectively, in each bin.\n",
    "\n",
    "##### Constraints:\n",
    "\n",
    "- The default rate per bin should be monotonically increasing or decreasing.\n",
    "- The bins should be statistically different (determined by the Chi-square test or another suitable hypothesis test with a chosen significance level).\n",
    "- The minimum number of observations per bin should be met.\n",
    "- The minimum and maximum number of bins should be within the specified range.\n",
    "\n",
    "##### Variables:\n",
    "\n",
    "- Bins as a set of ordinal categories.\n",
    "- Default rate: The proportion of default observations (default_flag = 1) in each bin.\n",
    "\n",
    "A similar problem is solved here: https://github.com/guillermo-navas-palencia/optbinning/blob/master/optbinning/binning/cp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a705988-f8af-4d21-9c58-65d1e832f310",
   "metadata": {},
   "source": [
    "**ChatGPT prompt one could use**\n",
    "\n",
    "Create CP model using ortools that solves the optimisation problem, where an input is a pandas dataframe consisting of a discrete explanatory variable with 50 bins and a binary target variable (default or not a default). The model should combine adjacent bins that maximise IV given that all constraints are satisified. IV must be calculated for every proposed combination of existing bins from stratch. The model should return a mapping table where all 50 bins are mapped to new categories. Focus onmake the code run.\n",
    "\n",
    "##### Maximize IV value of the bins\n",
    "\n",
    "IV = ∑((Good% - Bad%) * ln(Good% / Bad%)), where Good% and Bad% are the proportions of non-default and default observations, respectively, in each bin.\n",
    "\n",
    "##### Constraints:\n",
    "\n",
    "- The event rate (default rate) per bin should be monotonically increasing or decreasing.\n",
    "- The bins should be statistically different (determined by the Chi-square test or another suitable hypothesis test with a chosen significance level).\n",
    "- The minimum number of observations per bin should be met.\n",
    "- The minimum and maximum number of bins should be within the specified range.\n",
    "\n",
    "##### Variables:\n",
    "\n",
    "- Bins as a set of ordinal categories.\n",
    "- Default rate: The proportion of default observations (default_flag = 1) in each bin.\n",
    "\n",
    "\n",
    "##### Helper functions:\n",
    "\n",
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def calculate_iv(df, binned_column, target_column):\n",
    "    bin_summary = df.groupby(binned_column)[target_column].agg(['count', 'sum'])\n",
    "    bin_summary['non_target'] = bin_summary['count'] - bin_summary['sum']\n",
    "    \n",
    "    bin_summary['target_dist'] = bin_summary['sum'] / bin_summary['sum'].sum()\n",
    "    bin_summary['non_target_dist'] = bin_summary['non_target'] / bin_summary['non_target'].sum()\n",
    "    \n",
    "    bin_summary['woe'] = np.log(bin_summary['target_dist'] / bin_summary['non_target_dist'])\n",
    "    bin_summary['iv'] = (bin_summary['target_dist'] - bin_summary['non_target_dist']) * bin_summary['woe']\n",
    "    \n",
    "    return bin_summary['iv'].sum()\n",
    "\n",
    "\n",
    "def check_monotonic_constraint(df, binned_column, target_column):\n",
    "    # Calculate the default rate for each bin\n",
    "    default_rate = df.groupby(binned_column)[target_column].mean()\n",
    "\n",
    "    # Check if the difference between adjacent bins' default rates is either positive or negative\n",
    "    diff = np.diff(default_rate)\n",
    "\n",
    "    # Check if the constraint is satisfied (either all positive or all negative differences)\n",
    "    is_monotonic = np.all(diff >= 0) or np.all(diff <= 0)\n",
    "\n",
    "    return is_monotonic\n",
    "\n",
    "\n",
    "def calculate_statistical_difference(df, binned_column, target_column, significance_level=0.05):\n",
    "    bin_pairs_p_values = []\n",
    "    \n",
    "    unique_bins = df[binned_column].unique()\n",
    "    for i in range(len(unique_bins) - 1):\n",
    "        bin1 = unique_bins[i]\n",
    "        bin2 = unique_bins[i + 1]\n",
    "        \n",
    "        contingency_table = pd.crosstab(df[df[binned_column].isin([bin1, bin2])][binned_column], df[target_column])\n",
    "        _, p, _, _ = chi2_contingency(contingency_table)\n",
    "        \n",
    "        bin_pairs_p_values.append((bin1, bin2, p))\n",
    "    \n",
    "    return pd.DataFrame(bin_pairs_p_values, columns=['Bin1', 'Bin2', 'P-value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a9533-94a4-44c9-bab6-c50b3ead2e68",
   "metadata": {},
   "source": [
    "### A. Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9913a43e-ec8b-46ac-a5f4-cebe59c68825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   explanatory_variable  target_variable  year  economic_cycle  binned_column\n",
      "0              6.678090                0  2019        0.140130             33\n",
      "1              8.521948                1  2020        0.914430             42\n",
      "2              4.038421                1  2018        0.783868             20\n",
      "3              3.281745                1  2020        0.119363             16\n",
      "4              8.264844                1  2020        0.568544             41\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(n):\n",
    "    years = np.random.choice([2016, 2017, 2018, 2019, 2020, 2021], n)\n",
    "    economic_cycle = np.random.rand(n)\n",
    "    explanatory_variable = np.random.rand(n) * 10\n",
    "\n",
    "    # Add noise to the relationship between explanatory_variable and economic_cycle\n",
    "    noise = np.random.normal(0, 0.1, n)\n",
    "\n",
    "    # Define a non-trivial and random relationship between target and explanatory variable\n",
    "    z = (explanatory_variable / 10) * economic_cycle + noise\n",
    "    target_proba = 1 / (1 + np.exp(-z))  # Apply logistic function\n",
    "    target_variable = (target_proba > 0.5).astype(int)\n",
    "\n",
    "    data = {\n",
    "        'explanatory_variable': explanatory_variable,\n",
    "        'target_variable': target_variable,\n",
    "        'year': years,\n",
    "        'economic_cycle': economic_cycle\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Choose the number of observations and number of bins\n",
    "n = 200_000 \n",
    "num_bins = 50\n",
    "\n",
    "# Create dataset and digitize outcome\n",
    "df = generate_data(n)\n",
    "df['binned_column'] = pd.qcut(df['explanatory_variable'], num_bins, labels=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf57067-ebcd-4287-b65c-f6b849bfd62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.999969099974354"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df['explanatory_variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246a74c-874b-416a-a647-a973204b0cf9",
   "metadata": {},
   "source": [
    "### B. Ancilliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052f5d46-c752-4eb6-bcfb-aa59d2021ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_iv(df, binned_column, target_column):\n",
    "    bin_summary = df.groupby(binned_column)[target_column].agg(['count', 'sum'])\n",
    "    bin_summary['non_target'] = bin_summary['count'] - bin_summary['sum']\n",
    "    \n",
    "    bin_summary['target_dist'] = bin_summary['sum'] / bin_summary['sum'].sum()\n",
    "    bin_summary['non_target_dist'] = bin_summary['non_target'] / bin_summary['non_target'].sum()\n",
    "    \n",
    "    bin_summary['woe'] = np.log(bin_summary['target_dist'] / bin_summary['non_target_dist'])\n",
    "    bin_summary['iv'] = (bin_summary['target_dist'] - bin_summary['non_target_dist']) * bin_summary['woe']\n",
    "    \n",
    "    return bin_summary['iv'].sum()\n",
    "\n",
    "\n",
    "def check_monotonic_constraint(df, binned_column, target_column):\n",
    "    # Calculate the default rate for each bin\n",
    "    default_rate = df.groupby(binned_column)[target_column].mean()\n",
    "\n",
    "    # Check if the difference between adjacent bins' default rates is either positive or negative\n",
    "    diff = np.diff(default_rate)\n",
    "\n",
    "    # Check if the constraint is satisfied (either all positive or all negative differences)\n",
    "    is_monotonic = np.all(diff >= 0) or np.all(diff <= 0)\n",
    "\n",
    "    return is_monotonic\n",
    "\n",
    "\n",
    "def calculate_statistical_difference(df, binned_column, target_column, significance_level=0.05):\n",
    "    bin_pairs_p_values = []\n",
    "    \n",
    "    unique_bins = df[binned_column].unique()\n",
    "    for i in range(len(unique_bins) - 1):\n",
    "        bin1 = unique_bins[i]\n",
    "        bin2 = unique_bins[i + 1]\n",
    "        \n",
    "        contingency_table = pd.crosstab(df[df[binned_column].isin([bin1, bin2])][binned_column], df[target_column])\n",
    "        _, p, _, _ = chi2_contingency(contingency_table)\n",
    "        \n",
    "        bin_pairs_p_values.append((bin1, bin2, p))\n",
    "    \n",
    "    return pd.DataFrame(bin_pairs_p_values, columns=['Bin1', 'Bin2', 'P-value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd231f35-5bc9-40a3-8c8b-6a1f9a4000a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C. Implement using OR-tools\n",
    "\n",
    "**This is the gpt-generated output. The task is to fix it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af29bcfa-84a7-4463-ab65-ca75e21c0b35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CpModel' object has no attribute 'AddConstraint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m min_observations_per_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     53\u001b[0m significance_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m---> 55\u001b[0m new_categories \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_bins\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinned_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_observations_per_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m new_categories\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36moptimize_bins\u001b[1;34m(df, binned_column, target_column, min_bins, max_bins, min_observations_per_bin, significance_level)\u001b[0m\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39mAdd(\u001b[38;5;28msum\u001b[39m(df[binned_column] \u001b[38;5;241m==\u001b[39m i) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_observations_per_bin)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Add custom constraint for monotonic and statistical difference using a callback function\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAddConstraint\u001b[49m(merged_bins_satisfy_constraints_callback(df, binned_column, target_column, significance_level), \u001b[38;5;28mlist\u001b[39m(bin_vars\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Define the objective function (IV value)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m iv_expression \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CpModel' object has no attribute 'AddConstraint'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from ortools.sat.python import cp_model\n",
    "\n",
    "\n",
    "from ortools.sat.python import cp_model\n",
    "\n",
    "# Define a custom constraint callback function\n",
    "def merged_bins_satisfy_constraints_callback(df, binned_column, target_column, significance_level):\n",
    "    def callback(vars):\n",
    "        merged_bins = {i: vars[i].Value() for i in range(len(vars))}\n",
    "        return merged_bins_satisfy_constraints(df, binned_column, target_column, merged_bins, significance_level)\n",
    "    return callback\n",
    "\n",
    "def optimize_bins(df, binned_column, target_column, min_bins, max_bins, min_observations_per_bin, significance_level=0.05):\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    # Create a variable for each bin and its corresponding merged bin\n",
    "    bin_vars = {i: model.NewIntVar(0, max_bins - 1, f\"bin_{i}\") for i in range(50)}\n",
    "\n",
    "    # Add constraint: minimum number of observations per bin\n",
    "    for i in range(50):\n",
    "        model.Add(sum(df[binned_column] == i) >= min_observations_per_bin)\n",
    "\n",
    "    # Add custom constraint for monotonic and statistical difference using a callback function\n",
    "    model.AddConstraint(merged_bins_satisfy_constraints_callback(df, binned_column, target_column, significance_level), list(bin_vars.values()))\n",
    "\n",
    "    # Define the objective function (IV value)\n",
    "    iv_expression = []\n",
    "    for i in range(50):\n",
    "        df_temp = df[df[binned_column] == i]\n",
    "        iv_value = calculate_iv(df_temp, binned_column, target_column)\n",
    "        iv_expression.append(iv_value * bin_vars[i])\n",
    "\n",
    "    model.Maximize(sum(iv_expression))\n",
    "\n",
    "    # Solve the model\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.Solve(model)\n",
    "\n",
    "    # Return the mapping table\n",
    "    if status == cp_model.OPTIMAL:\n",
    "        new_categories = {i: solver.Value(bin_vars[i]) for i in range(50)}\n",
    "        return new_categories\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "min_bins = 10\n",
    "max_bins = 20\n",
    "min_observations_per_bin = 100\n",
    "significance_level = 0.05\n",
    "\n",
    "new_categories = optimize_bins(df, 'binned_column', 'target_column', min_bins, max_bins, min_observations_per_bin, significance_level)\n",
    "\n",
    "new_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6195fa-c5e7-4c2c-81f0-19c31d1fd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the monotinicity of the solution\n",
    "df.groupby(['bin']).agg({'target_variable':['mean','count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4848610-0966-47ad-b923-c8d949b4fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6b56a-2549-42cd-9977-57d37e4c77ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c35c2a-bf40-4b85-a76e-71746d6ed6b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "To determine whether adjacent bins are statistically different, you can perform a hypothesis test, such as the Chi-square test for independence. \n",
    "\n",
    "Here's a step-by-step process for doing this:\n",
    "- Create a contingency table for the adjacent bins, showing the frequency distribution of the binary default flag (0 and 1) for each pair of adjacent bins.\n",
    "- Calculate the expected frequency for each cell in the contingency table under the assumption that the bins are independent. To do this, multiply the row total (sum of binary default flags in a bin) by the column total (sum of observations in the two adjacent bins) and divide by the total number of observations.\n",
    "- Calculate the Chi-square statistic. For each cell, find the difference between the observed frequency and the expected frequency, square the result, and divide by the expected frequency. Add up all these values to get the Chi-square statistic.\n",
    "- Determine the degrees of freedom for the Chi-square test. For a 2x2 contingency table (2 rows, one for each binary default flag, and 2 columns, one for each adjacent bin), the degrees of freedom are (number of rows - 1) * (number of columns - 1) = 1.\n",
    "- Determine the critical value and the p-value for the calculated Chi-square statistic using the Chi-square distribution table or a statistical software with the corresponding degrees of freedom.\n",
    "- Set a significance level (commonly 0.05). If the p-value is less than the chosen significance level, reject the null hypothesis that the adjacent bins are independent, which means there is a statistically significant difference between the bins. If the p-value is greater than the chosen significance level, you cannot reject the null hypothesis, and there is no evidence to suggest a statistically significant difference between the bins.\n",
    "\n",
    "Repeat these steps for each pair of adjacent bins to determine whether there is a statistically significant difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2d4b7e-8f7f-4e80-8a94-c50e5a118740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test for A and B:\n",
      "Chi2 = 0.0000, p-value = 1.0000\n",
      "There is no evidence to suggest a statistically significant difference between bins A and B.\n",
      "\n",
      "Chi-square test for B and C:\n",
      "Chi2 = 0.0000, p-value = 1.0000\n",
      "There is no evidence to suggest a statistically significant difference between bins B and C.\n",
      "\n",
      "Chi-square test for C and D:\n",
      "Chi2 = 0.0000, p-value = 1.0000\n",
      "There is no evidence to suggest a statistically significant difference between bins C and D.\n",
      "\n",
      "Chi-square test for D and E:\n",
      "Chi2 = 0.0000, p-value = 1.0000\n",
      "There is no evidence to suggest a statistically significant difference between bins D and E.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'bin': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D', 'E', 'E', 'E'],\n",
    "    'default_flag': [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform Chi-square test for independence on adjacent bins\n",
    "significance_level = 0.05\n",
    "\n",
    "for i in range(len(df['bin'].unique()) - 1):\n",
    "    bin1 = df['bin'].unique()[i]\n",
    "    bin2 = df['bin'].unique()[i + 1]\n",
    "    contingency_table = pd.crosstab(df[df['bin'].isin([bin1, bin2])]['bin'], df['default_flag'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Chi-square test for {bin1} and {bin2}:\")\n",
    "    print(f\"Chi2 = {chi2:.4f}, p-value = {p:.4f}\")\n",
    "    \n",
    "    if p < significance_level:\n",
    "        print(f\"The difference between bins {bin1} and {bin2} is statistically significant.\\n\")\n",
    "    else:\n",
    "        print(f\"There is no evidence to suggest a statistically significant difference between bins {bin1} and {bin2}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38c85e-58b0-40ca-9afd-40be6b24f2a3",
   "metadata": {},
   "source": [
    "**This is a collection of related, but not working, code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf119d6b-7414-4a6e-98f4-0938072a4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = cp_model.CpModel()\n",
    "\n",
    "# Decision variables\n",
    "x, y, t, d, u, bin_size_diff = self.decision_variables(model, n)\n",
    "\n",
    "# Objective function\n",
    "total_records = int(n_records.sum())\n",
    "regularization = int(np.ceil(M * self.gamma / total_records))\n",
    "pmax = model.NewIntVar(0, total_records, \"pmax\")\n",
    "pmin = model.NewIntVar(0, total_records, \"pmin\")\n",
    "\n",
    "model.Maximize(sum([(V[i][i] * x[i, i]) +\n",
    "               sum([(V[i][j] - V[i][j+1]) * x[i, j]\n",
    "                    for j in range(i)]) for i in range(n)]) -\n",
    "               regularization * (pmax - pmin))\n",
    "\n",
    "# Constraint: unique assignment\n",
    "self.add_constraint_unique_assignment(model, n, x)\n",
    "\n",
    "# Constraint: min / max bins\n",
    "self.add_constraint_min_max_bins(model, n, x, d)\n",
    "\n",
    "# Constraint: min / max bin size\n",
    "self.add_constraint_min_max_bin_size(model, n, x, u, n_records,\n",
    "                                     bin_size_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ed6c8-32bd-4281-a1bc-58df58ed91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(divergence, n_nonevent, n_event, max_pvalue, max_pvalue_policy,\n",
    "               min_event_rate_diff, scale=None, return_nonevent_event=False):\n",
    "    n = len(n_nonevent)\n",
    "\n",
    "    t_n_event = n_event.sum()\n",
    "    t_n_nonevent = n_nonevent.sum()\n",
    "\n",
    "    D = []\n",
    "    V = []\n",
    "\n",
    "    E = []\n",
    "    NE = []\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        s_event = n_event[:i][::-1].cumsum()[::-1]\n",
    "        s_nonevent = n_nonevent[:i][::-1].cumsum()[::-1]\n",
    "        rate = s_event / (s_nonevent + s_event)\n",
    "\n",
    "        p = s_event / t_n_event\n",
    "        q = s_nonevent / t_n_nonevent\n",
    "\n",
    "        if divergence == \"iv\":\n",
    "            iv = jeffrey(p, q)\n",
    "        elif divergence == \"js\":\n",
    "            iv = jensen_shannon(p, q)\n",
    "        elif divergence == \"hellinger\":\n",
    "            iv = hellinger(p, q)\n",
    "        elif divergence == \"triangular\":\n",
    "            iv = triangular(p, q)\n",
    "\n",
    "        if scale is not None:\n",
    "            rate *= scale\n",
    "            iv *= scale\n",
    "\n",
    "            D.append(rate.astype(np.int64))\n",
    "            V.append(iv.astype(np.int64))\n",
    "        else:\n",
    "            D.append(rate)\n",
    "            V.append(iv)\n",
    "\n",
    "        if max_pvalue is not None or return_nonevent_event:\n",
    "            E.append(s_event)\n",
    "            NE.append(s_nonevent)\n",
    "\n",
    "    if max_pvalue is not None:\n",
    "        pvalue_violation_indices = find_pvalue_violation_indices(\n",
    "            n, E, NE, max_pvalue, max_pvalue_policy)\n",
    "    else:\n",
    "        pvalue_violation_indices = []\n",
    "\n",
    "    if min_event_rate_diff > 0:\n",
    "        if scale is not None:\n",
    "            min_diff = int(min_event_rate_diff * scale)\n",
    "        else:\n",
    "            min_diff = min_event_rate_diff\n",
    "\n",
    "        min_diff_violation_indices = find_min_diff_violation_indices(\n",
    "            n, D, min_diff)\n",
    "    else:\n",
    "        min_diff_violation_indices = []\n",
    "\n",
    "    if return_nonevent_event:\n",
    "        return D, V, NE, E, pvalue_violation_indices\n",
    "\n",
    "    return D, V, pvalue_violation_indices, min_diff_violation_indices\n",
    "\n",
    "def build_model(self, divergence, n_nonevent, n_event, trend_change):\n",
    "        # Parameters\n",
    "        M = int(1e6)\n",
    "        (D, V, pvalue_violation_indices,\n",
    "         min_diff_violation_indices) = model_data(\n",
    "            divergence, n_nonevent, n_event, self.max_pvalue,\n",
    "            self.max_pvalue_policy, self.min_event_rate_diff, M)\n",
    "\n",
    "        n = len(n_nonevent)\n",
    "        n_records = n_nonevent + n_event\n",
    "\n",
    "        # Initialize model\n",
    "        model = cp_model.CpModel()\n",
    "\n",
    "        # Decision variables\n",
    "        x, y, t, d, u, bin_size_diff = self.decision_variables(model, n)\n",
    "\n",
    "        # Objective function\n",
    "        if self.gamma:\n",
    "            total_records = int(n_records.sum())\n",
    "            regularization = int(np.ceil(M * self.gamma / total_records))\n",
    "            pmax = model.NewIntVar(0, total_records, \"pmax\")\n",
    "            pmin = model.NewIntVar(0, total_records, \"pmin\")\n",
    "\n",
    "            model.Maximize(sum([(V[i][i] * x[i, i]) +\n",
    "                           sum([(V[i][j] - V[i][j+1]) * x[i, j]\n",
    "                                for j in range(i)]) for i in range(n)]) -\n",
    "                           regularization * (pmax - pmin))\n",
    "        else:\n",
    "            model.Maximize(sum([(V[i][i] * x[i, i]) +\n",
    "                           sum([(V[i][j] - V[i][j+1]) * x[i, j]\n",
    "                                for j in range(i)]) for i in range(n)]))\n",
    "\n",
    "        # Constraint: min / max bins\n",
    "        self.add_constraint_min_max_bins(model, n, x, d)\n",
    "        \n",
    "def jeffrey(x, y, return_sum=False):\n",
    "    \"\"\"Calculate the Jeffrey's divergence between two distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Discrete probability distribution.\n",
    "\n",
    "    y : array-like\n",
    "        Discrete probability distribution.\n",
    "\n",
    "    return_sum : bool\n",
    "        Return sum of jeffrey values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jeffrey : float or numpy.ndarray\n",
    "    \"\"\"\n",
    "    x, y = _check_x_y(x, y)\n",
    "\n",
    "    j = special.xlogy(x - y, x / y)\n",
    "\n",
    "    if return_sum:\n",
    "        return j.sum()\n",
    "    else:\n",
    "        return j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
