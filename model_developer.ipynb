{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65b9480-9c16-4dce-9516-b2e5588dd5d2",
   "metadata": {},
   "source": [
    "# Credit Risk Modeling Pipeline Overview\n",
    "\n",
    "This document provides a high-level overview of an automated credit risk modeling pipeline. The pipeline is   \n",
    "designed to be intelligent and adaptable, using a GPT-powered agent to orchestrate each step. The core idea   \n",
    "is to build a model for predicting credit risk, whether that is a credit risk scoring model or a market risk model.   \n",
    "This involves data quality evaluation, univariate analysis, multivariate analysis including model selection,   \n",
    "model building, and calibration. The key components and flow are described below.\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "*   **GPTAgent:**\n",
    "    *   This is the brain of the pipeline. It's an AI agent that uses a large language model (LLM) to make decisions at each stage.\n",
    "    *   It generates analysis results, assesses their quality, creates dynamic action plans to guide the pipeline and selects the most suitable options.\n",
    "    *   It maintains a context of the pipeline's progress, which it uses to make informed decisions.\n",
    "*   **DataQualityChecker:**\n",
    "    *   This component checks and cleans raw data. It identifies missing values, outliers, and inconsistencies.\n",
    "    *   It stores the results of these checks, and it cleans the data.\n",
    "*   **UnivariateAnalyzer:**\n",
    "    *   Performs individual (univariate) analysis on each feature in the dataset.\n",
    "    *   It calculates the distribution statistics of features and presents multiple Weight of Evidence (WOE) binning options.\n",
    "*   **MultivariateAnalyzer:**\n",
    "    *   Analyzes relationships between different features in the dataset (multivariate analysis).\n",
    "    *   This step performs feature selection and, most importantly, fitting models such as logistic regression, OLS and XGBoost.\n",
    "    *   It returns multiple modeling options allowing the GPT agent to select the best.\n",
    "*   **Model:**\n",
    "    *   An abstract class for representing a model object.\n",
    "    *   Concrete classes are created to handle different types of models.\n",
    "*   **RiskModelBuilder:**\n",
    "    *   Builds and fits the chosen model and stores performance metrics.\n",
    "    *   It also generates plots of the ROC curve to analyze performance.\n",
    "*   **ModelCalibrator:**\n",
    "    *   Calibrates the predicted probabilities to ensure the model produces well-calibrated risk scores.\n",
    "*   **Documentor:**\n",
    "    *   Stores all the results and assessment from each step, and is responsible for generating a final report using LaTeX.\n",
    "*   **Assessor:**\n",
    "    *   Evaluates the results of each step, and flags unfavorable outcomes.\n",
    "    *   Works hand-in-hand with the `GPTAgent` to decide what the next step should be.\n",
    "\n",
    "## Pipeline Flow:\n",
    "\n",
    "The pipeline is designed to be non-linear. Here's a typical flow:\n",
    "\n",
    "1.  **Data Quality Check:** The `DataQualityChecker` cleans the raw data, storing all results and metrics.\n",
    "2.  **Univariate Analysis:** The `UnivariateAnalyzer` performs the univariate analysis of the features, returning multiple options to the GPT agent.\n",
    "3.  **Feature Selection and Multivariate Modeling:** The `MultivariateAnalyzer` selects features, and generates multiple model options. The GPT agent chooses the best option.\n",
    "4.  **Model Building and Evaluation:** The `RiskModelBuilder` fits the selected model, and generates evaluation metrics, including a ROC curve.\n",
    "5.  **Calibration:** The `ModelCalibrator` calibrates the model's predicted probabilities.\n",
    "\n",
    "Each step starts with the GPT agent creating a dynamic \"action plan\" on how the step should be conducted, and which functions   \n",
    "should be called, based on the current status and historical results. The agent assess the quality of results and, if needed,   \n",
    "triggers backtracking, specifying the next step to take.\n",
    "\n",
    "The process is iterative, and if the results of any step are not up to standard (according to the `Assessor`), the GPT agent   \n",
    "can decide to go back and re-run any of the steps. The feedback mechanisms and decision-making logic are mostly contained within   \n",
    "the `GPTAgent`, making the process intelligent and adaptable.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "*   **GPT-Driven Decision-Making:** The core logic and control is passed to the GPT agent, rather than pre-defined logic. This makes the process adaptable.\n",
    "*   **Non-Linear Flow:** The pipeline is non-linear and iterative, allowing the GPT agent to adapt to different challenges in the dataset.\n",
    "*   **Detailed Documentation:** Results and GPT comments are stored, and a report is generated with LaTeX.\n",
    "*   **Modular Design:** The pipeline is created using separate classes, making it easy to extend.\n",
    "*   **Context Management:** The GPT agent has access to the history of the pipeline and can make informed decisions on that basis.\n",
    "\n",
    "This framework provides a robust base for building and evaluating credit risk models, using AI to enhance both the intelligence and adaptability of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616451fc-062e-43db-9efa-3e631b51b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore, ks_2samp\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Tuple, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "import json\n",
    "\n",
    "\n",
    "class GPTAgent:\n",
    "    \"\"\"\n",
    "    A GPT-powered agent responsible for orchestrating the credit risk modeling pipeline.\n",
    "\n",
    "    This agent acts as the decision-making core of the pipeline, leveraging\n",
    "    a large language model (LLM) to:\n",
    "        - Generate human-readable commentary on analysis results.\n",
    "        - Assess the quality and suitability of results against predefined metrics.\n",
    "        - Create dynamic action plans to guide the pipeline execution.\n",
    "        - Select the most appropriate options from multiple choices.\n",
    "        - Provide reasoning for backtracking and course correction.\n",
    "        - Maintain a context of the pipeline's progress.\n",
    "\n",
    "    The agent is designed to be adaptable, configurable and maintain its context,\n",
    "    making it suitable for integration with various model training and data analysis pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "        self.prompt_templates = self._load_prompt_templates()\n",
    "        self.knowledge_base = self._load_knowledge_base()  # Placeholder for the knowledge base.\n",
    "\n",
    "    def _load_prompt_templates(self) -> Dict:\n",
    "        \"\"\"Loads prompt templates from a JSON file\"\"\"\n",
    "        try:\n",
    "            with open('prompt_templates.json', 'r') as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Error loading prompt templates: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _load_knowledge_base(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Loads the knowledge base with rules for analysis and model building.\n",
    "        \"\"\"\n",
    "        # Mock knowledge base\n",
    "        return {\n",
    "            \"feature_selection_rules\": {\n",
    "                \"high_cardinality\": \"Consider feature selection with VIF to avoid multicollinearity.\",\n",
    "                \"high_correlation\": \"Perform feature selection with correlation to reduce redundancy.\"\n",
    "            },\n",
    "            \"model_choice_rules\": {\n",
    "                \"binary_classification\": [\"logistic_regression\", \"xgboost\"],\n",
    "                \"regression\": [\"ols\"],\n",
    "                \"small_dataset\": \"Consider simpler models or regularization.\",\n",
    "                \"large_dataset\": \"Consider more complex models.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _call_llm(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Placeholder for the actual LLM API call\n",
    "        \"\"\"\n",
    "        # Mock LLM response\n",
    "        print(\"LLM called with prompt:\", prompt)\n",
    "        return f\"LLM response for prompt: {prompt}\"\n",
    "\n",
    "    def generate_commentary(self, step_name: str, results: Dict, metrics: Dict, template: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates commentary based on the results of a given step.\n",
    "        \"\"\"\n",
    "        prompt = self.prompt_templates.get(\"commentary\", \"\").format(\n",
    "            step_name=step_name, results=results, metrics=metrics, template=template\n",
    "        )\n",
    "        response = self._call_llm(prompt)\n",
    "        # Mock response parsing\n",
    "        return f\"GPT Commentary: {response}\"\n",
    "\n",
    "    def assess_results(self, step_name: str, results: Dict, metrics: Dict, template: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Assesses results of a given step and provides a recommendation.\n",
    "        \"\"\"\n",
    "        prompt = self.prompt_templates.get(\"assessment\", \"\").format(\n",
    "            step_name=step_name, results=results, metrics=metrics, template=template\n",
    "        )\n",
    "        response = self._call_llm(prompt)\n",
    "        # Mock assessment logic\n",
    "        if 'auc' in results and results['auc'] < 0.6:\n",
    "            return False, \"AUC is below 0.6, consider re-running feature selection or data preparation.\"\n",
    "        if 'ks' in results and results['ks'] < 0.15:\n",
    "            return False, \"KS is below 0.15, consider re-running feature selection or data preparation.\"\n",
    "        return True, \"Results are acceptable.\"\n",
    "\n",
    "    def create_action_plan(self, step_name: str, context: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "         Generates an action plan for a specific step. The plan includes function names and parameters.\n",
    "        \"\"\"\n",
    "        if step_name == \"data_quality\":\n",
    "           return {\"action\": \"run_checks\", \"params\": {}}\n",
    "        elif step_name == \"univariate_analysis\":\n",
    "            features = context.get(\"features\", [])\n",
    "            return {\"action\": \"calculate_distribution\", \"params\": {\"features\": features}}\n",
    "        elif step_name == \"feature_selection\":\n",
    "          features = context.get(\"features\", [])\n",
    "          \n",
    "          # Mock Feature Selection Plan Logic\n",
    "          vif_required = False\n",
    "          if len(features) > 5:\n",
    "            vif_required = True\n",
    "          \n",
    "          rule_based_options = []\n",
    "          if vif_required:\n",
    "             rule_based_options.append(\"vif\")\n",
    "          \n",
    "          if context.get('previous_step') == 'model_evaluation' or context.get('previous_step') == 'calibration':\n",
    "            rule_based_options.append(\"correlation\")\n",
    "          \n",
    "          if not rule_based_options:\n",
    "            rule_based_options = [\"correlation\", \"vif\"] # Adding default\n",
    "\n",
    "          return {\"action\": \"feature_selection_options\", \"params\": {\"features\": features, \"options\": rule_based_options,\"threshold\": 0.7}}\n",
    "\n",
    "        elif step_name == \"model_evaluation\":\n",
    "          X = context.get(\"X\", [])\n",
    "          y = context.get(\"y\", [])\n",
    "          features = context.get(\"features\", [])\n",
    "\n",
    "          # Model Choice Logic\n",
    "          model_options = self.knowledge_base[\"model_choice_rules\"].get(\"binary_classification\", [])\n",
    "\n",
    "          if len(X) > 100:\n",
    "              if \"ols\" in model_options:\n",
    "                model_options.remove(\"ols\")\n",
    "\n",
    "          return {\"action\": \"model_options\", \"params\": {\"X\": X, \"y\": y, \"features\": features, \"options\": model_options}}\n",
    "\n",
    "        elif step_name == \"calibration\":\n",
    "             preds = context.get(\"preds\", [])\n",
    "             y_true = context.get(\"y_true\", [])\n",
    "             return {\"action\": \"fit_calibrator\", \"params\": {\"preds\": preds, \"y_true\": y_true}}\n",
    "        return {}\n",
    "\n",
    "    def choose_option(self, options: Dict, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Chooses one option from multiple ones based on context.\n",
    "        \"\"\"\n",
    "        prompt = self.prompt_templates.get(\"choice\", \"\").format(options=options, context=context)\n",
    "        response = self._call_llm(prompt)\n",
    "        # Mock choice parsing\n",
    "        print(\"LLM choise:\", response)\n",
    "        return list(options.keys())[0]\n",
    "\n",
    "    def generate_backtracking_comment(self, step_name: str, reason: str) -> str:\n",
    "         \"\"\"\n",
    "         Generates a comment for backtracking.\n",
    "        \"\"\"\n",
    "         prompt = self.prompt_templates.get(\"backtracking\", \"\").format(step_name=step_name, reason=reason)\n",
    "         response = self._call_llm(prompt)\n",
    "         return f\"GPT Backtracking: {response}\"\n",
    "\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Checks and cleans raw data, stores metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, documentor: 'Documentor', gpt_agent: GPTAgent = None):\n",
    "        self.df = df\n",
    "        self.documentor = documentor\n",
    "        self.gpt_agent = gpt_agent\n",
    "        self.results = {}\n",
    "        self.assessment = {}\n",
    "\n",
    "    def run_checks(self) -> Dict:\n",
    "        \"\"\"\n",
    "         Performs a variety of data quality checks and returns the results as a dictionary.\n",
    "         \"\"\"\n",
    "        self.results = {}\n",
    "        self.results['missing_values'] = self.df.isnull().sum().to_dict()\n",
    "        self.results['dtypes'] = self.df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        outlier_counts = (zscore(self.df.select_dtypes(include='number')).abs() > 3).sum()\n",
    "        self.results['outliers'] = outlier_counts.to_dict()\n",
    "        \n",
    "        if not self.df.empty:\n",
    "          for col in self.df.columns:\n",
    "              if self.df[col].dtype in ['int64', 'float64']:\n",
    "                 self.results[f'{col}_stats'] = self.df[col].describe().to_dict()\n",
    "              elif self.df[col].dtype == 'object':\n",
    "                 self.results[f'{col}_unique'] = len(self.df[col].unique())\n",
    "        return self.results\n",
    "\n",
    "    def clean_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs cleaning of the dataframe: dropping rows with many NaNs and imputing the rest\n",
    "        \"\"\"\n",
    "        self.df.dropna(thresh=len(self.df.columns) - 2, inplace=True)\n",
    "        for col in self.df.columns:\n",
    "          if self.df[col].dtype in ['int64', 'float64']:\n",
    "             self.df[col].fillna(self.df[col].median(), inplace=True)\n",
    "          elif self.df[col].dtype == 'object':\n",
    "             self.df[col].fillna(self.df[col].mode()[0], inplace=True)\n",
    "        return self.df\n",
    "\n",
    "\n",
    "class UnivariateAnalyzer:\n",
    "    \"\"\"\n",
    "    Performs univariate analysis, returns multiple options and lets GPT comment.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, target_col: str, documentor: 'Documentor', gpt_agent: GPTAgent = None):\n",
    "      self.df = df\n",
    "      self.target_col = target_col\n",
    "      self.documentor = documentor\n",
    "      self.gpt_agent = gpt_agent\n",
    "      self.results = {}\n",
    "      self.assessment = {}\n",
    "\n",
    "    def calculate_distribution(self, features: List) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculates the distribution statistics of the input features.\n",
    "        \"\"\"\n",
    "        distribution_results = {}\n",
    "        for feature in features:\n",
    "            if self.df[feature].dtype in ['int64', 'float64']:\n",
    "                distribution_results[feature] = self.df[feature].describe().to_dict()\n",
    "            elif self.df[feature].dtype == 'object':\n",
    "                value_counts = self.df[feature].value_counts()\n",
    "                distribution_results[feature] = {\n",
    "                    'counts': value_counts.to_dict(),\n",
    "                    'probabilities': (value_counts / len(self.df)).to_dict()\n",
    "                }\n",
    "        self.results['distribution'] = distribution_results\n",
    "        return distribution_results\n",
    "\n",
    "    def woe_options(self, feature: str, bins: int = 5) -> Tuple[Dict, str]:\n",
    "        \"\"\"\n",
    "         Calculates multiple Weight of Evidence (WOE) binning options and selects one.\n",
    "         \"\"\"\n",
    "        options = {\n",
    "            \"option_1\": self._calculate_woe(feature, bins),\n",
    "            \"option_2\": self._calculate_woe(feature, bins + 1)\n",
    "        }\n",
    "        chosen = \"option_1\"  # Arbitrary choice to show the functionality\n",
    "        self.results[f\"{feature}_woe_choices\"] = options\n",
    "        self.results[f\"{feature}_woe_chosen\"] = chosen\n",
    "        return options, chosen\n",
    "        \n",
    "    def _calculate_woe(self, feature: str, bins: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculates Weight of Evidence (WOE) values for a given feature and a specific number of bins.\n",
    "        \"\"\"\n",
    "        if self.df[feature].dtype in ['int64', 'float64']:\n",
    "          # Bin numerical features\n",
    "          df_bins = pd.cut(self.df[feature], bins=bins, include_lowest=True)\n",
    "        elif self.df[feature].dtype == 'object':\n",
    "          df_bins = self.df[feature]\n",
    "        else:\n",
    "          raise ValueError(f\"Feature '{feature}' has an unsupported dtype: {self.df[feature].dtype}\")\n",
    "\n",
    "        df_temp = pd.DataFrame({'bins': df_bins, 'target': self.df[self.target_col]})\n",
    "        grouped = df_temp.groupby('bins')['target'].agg(['count', 'sum']).reset_index()\n",
    "        grouped.rename(columns={'count': 'total', 'sum': 'positives'}, inplace=True)\n",
    "        \n",
    "        grouped['negatives'] = grouped['total'] - grouped['positives']\n",
    "\n",
    "        total_positives = grouped['positives'].sum()\n",
    "        total_negatives = grouped['negatives'].sum()\n",
    "\n",
    "        grouped['pos_dist'] = grouped['positives'] / total_positives\n",
    "        grouped['neg_dist'] = grouped['negatives'] / total_negatives\n",
    "\n",
    "        # Avoid division by zero\n",
    "        grouped['woe'] = np.log(grouped['pos_dist'] / grouped['neg_dist'])\n",
    "        grouped['woe'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        grouped['woe'].fillna(0, inplace=True)\n",
    "\n",
    "        # Format output for clarity\n",
    "        woe_dict = {}\n",
    "        for _, row in grouped.iterrows():\n",
    "          if isinstance(row['bins'], pd.Interval): # Check if row['bins'] is an interval\n",
    "            label = f\"[{row['bins'].left:.2f}, {row['bins'].right:.2f}]\"\n",
    "          else:\n",
    "            label = row['bins']\n",
    "          woe_dict[label] = row['woe']\n",
    "            \n",
    "        return woe_dict\n",
    "\n",
    "\n",
    "class MultivariateAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes relationships among features, returns options, lets GPT choose.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, target_col: str, documentor: 'Documentor', gpt_agent: GPTAgent = None):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.documentor = documentor\n",
    "        self.gpt_agent = gpt_agent\n",
    "        self.results = {}\n",
    "        self.assessment = {}\n",
    "\n",
    "    def correlation_analysis(self, features: List) -> pd.DataFrame:\n",
    "       \"\"\"\n",
    "       Calculates and returns the correlation matrix for the specified features\n",
    "       \"\"\"\n",
    "       corr = self.df[features].corr()\n",
    "       self.results['correlation_matrix'] = corr.to_dict()\n",
    "       return corr\n",
    "    \n",
    "    def vif_analysis(self, features: List) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculates Variance Inflation Factor (VIF) for the given features and returns as a dictionary.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        \n",
    "        vif_data = {}\n",
    "        \n",
    "        if not features:\n",
    "            return {}  # Return empty if no features provided\n",
    "\n",
    "        try:\n",
    "            X = self.df[features]\n",
    "            for i, feature in enumerate(features):\n",
    "                vif = variance_inflation_factor(X.values, i)\n",
    "                vif_data[feature] = vif\n",
    "            self.results['vif_analysis'] = vif_data\n",
    "            return vif_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error in VIF calculation: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def feature_selection_options(self, features: List, options: List, threshold: float = 0.7) -> Dict:\n",
    "       \"\"\"\n",
    "       Generates multiple feature selection options for GPT to choose from\n",
    "       \"\"\"\n",
    "       available_options = {}\n",
    "       if 'correlation' in options:\n",
    "           available_options['correlation'] = self._select_features_by_correlation(features, threshold)\n",
    "       if 'vif' in options:\n",
    "          available_options['vif'] = self._select_features_by_vif(features, threshold)\n",
    "       self.results['feature_selection_options'] = available_options\n",
    "       return available_options\n",
    "\n",
    "\n",
    "    def _select_features_by_correlation(self, features: List, threshold: float) -> List:\n",
    "        \"\"\"\n",
    "        Selects features based on correlation matrix.\n",
    "        \"\"\"\n",
    "        if not features:\n",
    "          return []\n",
    "        corr = self.df[features].corr()\n",
    "        selected_features = []\n",
    "        considered_features = set()\n",
    "        \n",
    "        for col1 in corr.columns:\n",
    "            if col1 not in considered_features:\n",
    "              selected_features.append(col1)\n",
    "              for col2 in corr.columns:\n",
    "                if col1 != col2 and abs(corr.loc[col1, col2]) > threshold:\n",
    "                  considered_features.add(col2)\n",
    "        return selected_features\n",
    "\n",
    "    def _select_features_by_vif(self, features: List, threshold: float) -> List:\n",
    "        \"\"\"\n",
    "        Selects features based on VIF values.\n",
    "        \"\"\"\n",
    "        vif_values = self.vif_analysis(features)\n",
    "        if not vif_values:\n",
    "          return []\n",
    "        return [feature for feature, vif in vif_values.items() if vif <= threshold]\n",
    "    \n",
    "    def model_options(self, X: pd.DataFrame, y: pd.Series, features: List, options: List) -> Dict:\n",
    "        \"\"\"\n",
    "         Generates multiple model options for GPT to choose from.\n",
    "        \"\"\"\n",
    "        available_options = {}\n",
    "        if 'logistic_regression' in options:\n",
    "          available_options[\"logistic_regression\"] = self._fit_logistic_regression(X, y, features)\n",
    "        if 'ols' in options:\n",
    "          available_options[\"ols\"] = self._fit_ols(X, y, features)\n",
    "        if 'xgboost' in options:\n",
    "          available_options[\"xgboost\"] = self._fit_xgboost(X, y, features)\n",
    "        self.results['model_options'] = available_options\n",
    "        return available_options\n",
    "\n",
    "    def _fit_logistic_regression(self, X: pd.DataFrame, y: pd.Series, features: List) -> Dict:\n",
    "       \"\"\"\n",
    "       Fits a logistic regression model using statsmodels.\n",
    "       \"\"\"\n",
    "       X = sm.add_constant(X) # Adding a constant\n",
    "       model = sm.Logit(y, X).fit(disp=0)\n",
    "       return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"summary\": model.summary().as_text(),\n",
    "        \"aic\": model.aic,\n",
    "        \"bic\": model.bic\n",
    "        }\n",
    "\n",
    "    def _fit_ols(self, X: pd.DataFrame, y: pd.Series, features: List) -> Dict:\n",
    "       \"\"\"\n",
    "       Fits an OLS model using statsmodels.\n",
    "       \"\"\"\n",
    "       X = sm.add_constant(X) # Adding a constant\n",
    "       model = sm.OLS(y, X).fit(disp=0)\n",
    "       return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"summary\": model.summary().as_text(),\n",
    "         \"aic\": model.aic,\n",
    "         \"bic\": model.bic\n",
    "        }\n",
    "    \n",
    "    def _fit_xgboost(self, X: pd.DataFrame, y: pd.Series, features: List) -> Dict:\n",
    "        \"\"\"Fits an XGBoost model.\"\"\"\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "        model.fit(X, y)\n",
    "        return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        }\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"Abstract base class for models.\"\"\"\n",
    "    @abstractmethod\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"Fit the model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      \"\"\"Make predictions\"\"\"\n",
    "      pass\n",
    "\n",
    "class LogisticRegressionModel(Model):\n",
    "  \"\"\"\n",
    "  Concrete class that implements Logistic Regression (sklearn)\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    self.model.fit(X, y)\n",
    "\n",
    "  def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "    return self.model.predict_proba(X)[:, 1]\n",
    "  \n",
    "  def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      return self.model.predict(X)\n",
    "\n",
    "\n",
    "class StatsModelWrapper(Model):\n",
    "    \"\"\"\n",
    "    Wraps a statsmodels model for consistent interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        # Statsmodels fit is done in the multivariate step. Nothing to do here.\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      \"\"\"Get probabilities using statsmodels object.\"\"\"\n",
    "      X = sm.add_constant(X)\n",
    "      return self.model.predict(X).values\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      \"\"\"Get labels using statsmodels object.\"\"\"\n",
    "      X = sm.add_constant(X)\n",
    "      return (self.model.predict(X).values > 0.5).astype(int) # Using default threshold of 0.5\n",
    "    \n",
    "class XGBoostModelWrapper(Model):\n",
    "    \"\"\"\n",
    "     Wraps an XGBoost model for consistent interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        # XGBoost fit is done in the multivariate step. Nothing to do here.\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      \"\"\"Get probabilities using xgboost object.\"\"\"\n",
    "      return self.model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "      \"\"\"Get labels using xgboost object.\"\"\"\n",
    "      return self.model.predict(X)\n",
    "\n",
    "\n",
    "class RiskModelBuilder:\n",
    "    \"\"\"\n",
    "    Builds and fits the model, stores performance metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: Model, documentor: 'Documentor', gpt_agent: GPTAgent = None):\n",
    "        self.model = model\n",
    "        self.documentor = documentor\n",
    "        self.gpt_agent = gpt_agent\n",
    "        self.results = {}\n",
    "        self.assessment = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, y_true: pd.Series, y_pred: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluates the model performance.\n",
    "        \"\"\"\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        ks = max(tpr - fpr)\n",
    "        accuracy = accuracy_score(y_true, (y_pred > 0.5).astype(int))\n",
    "        precision = precision_score(y_true, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        recall = recall_score(y_true, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        f1 = f1_score(y_true, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        self.results['auc'] = auc\n",
    "        self.results['ks'] = ks\n",
    "        self.results['accuracy'] = accuracy\n",
    "        self.results['precision'] = precision\n",
    "        self.results['recall'] = recall\n",
    "        self.results['f1'] = f1\n",
    "        self.results['fpr'] = fpr.tolist()\n",
    "        self.results['tpr'] = tpr.tolist()\n",
    "        self.results['thresholds'] = thresholds.tolist()\n",
    "        return self.results\n",
    "    \n",
    "    def plot_roc_curve(self, fpr: List, tpr: List) -> str:\n",
    "        \"\"\"Generates and saves ROC curve plot\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', label='ROC Curve')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig('roc_curve.png') # Save figure to a file\n",
    "        plt.close()  # Close the plot\n",
    "        return \"roc_curve.png\"\n",
    "\n",
    "\n",
    "class ModelCalibrator:\n",
    "    \"\"\"\n",
    "    Calibrates predicted probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: str = 'isotonic', documentor: 'Documentor', gpt_agent: GPTAgent = None):\n",
    "        self.method = method\n",
    "        self.calibrator = IsotonicRegression(out_of_bounds='clip') if method == 'isotonic' else None\n",
    "        self.documentor = documentor\n",
    "        self.gpt_agent = gpt_agent\n",
    "        self.results = {}\n",
    "        self.assessment = {}\n",
    "\n",
    "    def fit_calibrator(self, preds: np.ndarray, y_true: pd.Series) -> None:\n",
    "        if self.calibrator:\n",
    "            self.calibrator.fit(preds, y_true)\n",
    "\n",
    "    def calibrate(self, preds: np.ndarray) -> np.ndarray:\n",
    "        if self.calibrator:\n",
    "            return self.calibrator.transform(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "class Documentor:\n",
    "    \"\"\"\n",
    "    Handles storage of results and LaTeX report generation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sections = []\n",
    "        self.pipeline_results = {}\n",
    "\n",
    "    def add_section(self, title: str, content: str) -> None:\n",
    "        self.sections.append((title, content))\n",
    "    \n",
    "    def store_results(self, results: Dict, step_name: str) -> None:\n",
    "        self.pipeline_results[step_name] = results\n",
    "    \n",
    "    def get_results(self) -> Dict:\n",
    "        return self.pipeline_results\n",
    "    \n",
    "    def get_step_results(self, step_name: str) -> Dict:\n",
    "        return self.pipeline_results.get(step_name, {})\n",
    "    \n",
    "    def build_latex_document(self) -> str:\n",
    "        doc = r\"\\documentclass{article}\\usepackage{graphicx}\\begin{document}\"\n",
    "        for title, content in self.sections:\n",
    "            doc += f\"\\n\\\\section*{{{title}}}\\n{content}\\n\"\n",
    "        doc += r\"\\end{document}\"\n",
    "        return doc\n",
    "    \n",
    "    def extract_step_data(self, step_name: str, data_keys: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "         Extracts only selected data from the results of a particular step\n",
    "        \"\"\"\n",
    "        step_results = self.pipeline_results.get(step_name, {})\n",
    "        if not data_keys:\n",
    "           return step_results\n",
    "        return {key: step_results.get(key) for key in step_results if key in step_results}\n",
    "\n",
    "\n",
    "class Assessor:\n",
    "  \"\"\"\n",
    "  Evaluates the results of each step, uses GPT commentary, and checks for feedback.\n",
    "  \"\"\"\n",
    "  def __init__(self, documentor: 'Documentor', gpt_agent: GPTAgent, feedback_metrics: Dict = None):\n",
    "    self.documentor = documentor\n",
    "    self.gpt_agent = gpt_agent\n",
    "    self.feedback_metrics = feedback_metrics if feedback_metrics else {}\n",
    "\n",
    "  def assess_step(self, step_name: str, metrics: Dict = None, template: str = None) -> Tuple[bool, str]:\n",
    "      \"\"\"\n",
    "      Assesses the results of a pipeline step, returns True if assessment is favorable, otherwise False and suggests the next step.\n",
    "      \"\"\"\n",
    "      results = self.documentor.get_step_results(step_name)\n",
    "      if not results:\n",
    "         print(f\"No results found for {step_name}\")\n",
    "         return True, \"\"\n",
    "\n",
    "      assessment_result, assessment_commentary = self.gpt_agent.assess_results(\n",
    "                  step_name=step_name,\n",
    "                  results=results,\n",
    "                  metrics=metrics,\n",
    "                  template=template\n",
    "      )\n",
    "      commentary = self.gpt_agent.generate_commentary(\n",
    "                  step_name=step_name,\n",
    "                  results=results,\n",
    "                  metrics=metrics,\n",
    "                  template=template\n",
    "      )\n",
    "      self.documentor.add_section(f\"GPT Commentary on {step_name}\", commentary)\n",
    "      self.documentor.add_section(f\"Assessment Result for {step_name}\", f\"{assessment_result}. Next Steps: {assessment_commentary}\")\n",
    "      if assessment_result is False:\n",
    "        print(f\"Assessment failed for {step_name}: {assessment_commentary}\")\n",
    "      return assessment_result, assessment_commentary\n",
    "\n",
    "\n",
    "class CreditRiskPipeline:\n",
    "  \"\"\"\n",
    "  Orchestrates the entire credit risk modeling pipeline, data quality to calibration.\n",
    "  \"\"\"\n",
    "  def __init__(self, df: pd.DataFrame, model: Model, target_col: str, gpt_agent: GPTAgent = None,\n",
    "               test_size: float = 0.2, random_state: int = 42, feedback_metrics: Dict = None):\n",
    "      self.df = df\n",
    "      self.target_col = target_col\n",
    "      self.gpt_agent = gpt_agent\n",
    "      self.test_size = test_size\n",
    "      self.random_state = random_state\n",
    "      self.documentor = Documentor()\n",
    "      self.assessor = Assessor(self.documentor, gpt_agent, feedback_metrics)\n",
    "\n",
    "      self.data_quality_checker = DataQualityChecker(df, self.documentor, gpt_agent)\n",
    "      self.univariate_analyzer = UnivariateAnalyzer(df, target_col, self.documentor, gpt_agent)\n",
    "      self.multivariate_analyzer = MultivariateAnalyzer(df, target_col, self.documentor, gpt_agent)\n",
    "      self.model_builder = RiskModelBuilder(model, self.documentor, gpt_agent)\n",
    "      self.calibrator = ModelCalibrator(documentor=self.documentor, gpt_agent=gpt_agent)\n",
    "\n",
    "      self.pipeline_results = {}\n",
    "\n",
    "\n",
    "  def run_pipeline(self, features: List, metrics: Dict = None, template: str = None, max_iterations: int = 2) -> Dict:\n",
    "    \"\"\"Runs the entire credit risk pipeline\"\"\"\n",
    "    current_step = \"data_quality\"  # Start with the data quality check\n",
    "    \n",
    "    history = []\n",
    "    context = {}\n",
    "    \n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "      print(f\"\\nRunning Pipeline - Iteration {iteration + 1}. Current Step: {current_step}\")\n",
    "      history.append(current_step)\n",
    "      context[\"history\"] = history # Adding history\n",
    "      train_df, test_df = train_test_split(self.df, test_size = self.test_size, random_state=self.random_state)\n",
    "      \n",
    "      current_state = {\n",
    "          \"features\": features,\n",
    "           \"X\": None,\n",
    "            \"y\": None,\n",
    "           \"preds\": None,\n",
    "            \"y_true\": None,\n",
    "           \"clean_train_df\": None\n",
    "      }\n",
    "      context.update(current_state)\n",
    "      \n",
    "      if current_step == \"data_quality\":\n",
    "          # Get Action Plan\n",
    "          action_plan = self.gpt_agent.create_action_plan(current_step, context)\n",
    "          # Step 1: Data Quality\n",
    "          if action_plan.get(\"action\") == \"run_checks\":\n",
    "            dq_metrics = self.data_quality_checker.run_checks()\n",
    "            self.documentor.store_results(dq_metrics, \"data_quality\")\n",
    "            clean_train_df = self.data_quality_checker.clean_data()\n",
    "            context[\"clean_train_df\"] = clean_train_df\n",
    "            context[\"data_quality_metrics\"] = dq_metrics\n",
    "          assessment_result, assessment_commentary = self.assessor.assess_step(\"data_quality\", metrics, template)\n",
    "          if not assessment_result:\n",
    "              current_step = \"data_quality\"\n",
    "              comment = self.gpt_agent.generate_backtracking_comment(current_step, assessment_commentary)\n",
    "              self.documentor.add_section(f\"Backtracking to {current_step}\", comment)\n",
    "              context[\"previous_step\"] = current_step\n",
    "              continue # Try another iteration\n",
    "          else:\n",
    "              current_step = \"univariate_analysis\"\n",
    "\n",
    "      elif current_step == \"univariate_analysis\":\n",
    "          # Get Action Plan\n",
    "          action_plan = self.gpt_agent.create_action_plan(current_step, context)\n",
    "          # Step 2: Univariate Analysis\n",
    "          if action_plan.get(\"action\") == \"calculate_distribution\":\n",
    "            distribution = self.univariate_analyzer.calculate_distribution(action_plan[\"params\"].get(\"features\"))\n",
    "            self.documentor.store_results(distribution, \"univariate_analysis\")\n",
    "            \n",
    "            if features:\n",
    "              woe_opts, chosen_opt = self.univariate_analyzer.woe_options(features[0])\n",
    "            else:\n",
    "                woe_opts, chosen_opt = {}, None\n",
    "            self.documentor.store_results({\"woe_options\": woe_opts, \"chosen_woe\": chosen_opt}, \"univariate_analysis_woe\")\n",
    "          \n",
    "          assessment_result, assessment_commentary = self.assessor.assess_step(\"univariate_analysis\", metrics, template)\n",
    "          if not assessment_result:\n",
    "              current_step = \"univariate_analysis\"\n",
    "              comment = self.gpt_agent.generate_backtracking_comment(current_step, assessment_commentary)\n",
    "              self.documentor.add_section(f\"Backtracking to {current_step}\", comment)\n",
    "              context[\"previous_step\"] = current_step\n",
    "              continue # Try another iteration\n",
    "          else:\n",
    "             current_step = \"feature_selection\"\n",
    "\n",
    "      elif current_step == \"feature_selection\":\n",
    "        # Get Action Plan\n",
    "        action_plan = self.gpt_agent.create_action_plan(current_step, context)\n",
    "        \n",
    "        # Step 3: Multivariate Analysis\n",
    "        if action_plan.get(\"action\") == \"feature_selection_options\":\n",
    "           corr = self.multivariate_analyzer.correlation_analysis(action_plan[\"params\"].get(\"features\"))\n",
    "           self.documentor.store_results(corr.to_dict(), \"correlation_matrix\")\n",
    "\n",
    "           feature_options = self.multivariate_analyzer.feature_selection_options(action_plan[\"params\"].get(\"features\"), action_plan[\"params\"].get(\"options\"), action_plan[\"params\"].get(\"threshold\"))\n",
    "           chosen_feature_method = self.gpt_agent.choose_option(feature_options, \"Choose best feature selection method\")\n",
    "        \n",
    "        assessment_result, assessment_commentary = self.assessor.assess_step(\"feature_selection\", metrics, template)\n",
    "        if not assessment_result:\n",
    "            if 're-run feature selection' in assessment_commentary:\n",
    "                current_step = \"feature_selection\"\n",
    "            elif 'data preparation' in assessment_commentary:\n",
    "                current_step = \"data_quality\"\n",
    "            else:\n",
    "                current_step = \"univariate_analysis\"\n",
    "            comment = self.gpt_agent.generate_backtracking_comment(current_step, assessment_commentary)\n",
    "            self.documentor.add_section(f\"Backtracking to {current_step}\", comment)\n",
    "            continue\n",
    "        else:\n",
    "            current_step = \"model_evaluation\"\n",
    "\n",
    "      elif current_step == \"model_evaluation\":\n",
    "          # Get Action Plan\n",
    "          action_plan = self.gpt_agent.create_action_plan(current_step, current_state)\n",
    "          # Step 4: Model Building\n",
    "          X_train = current_state[\"X\"]\n",
    "          y_train = current_state[\"y\"]\n",
    "          X_test, y_test = test_df[current_state[\"features\"]], test_df[self.target_col]\n",
    "          \n",
    "          model_type = self.documentor.get_step_results(\"multivariate_modelling\").get(\"chosen_model_option\")\n",
    "          chosen_model = model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
