{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85cb85e-d4bb-4e30-9dbd-4aea333ad553",
   "metadata": {},
   "source": [
    "## Generate documents\n",
    "\n",
    "This Jupyter cell contains Python code designed to automatically generate a book in LaTeX format.  It leverages several key technologies to streamline the process:\n",
    "\n",
    "*   **OpenAI's language models (like GPT-4o):** To generate the actual content of each book section based on a defined outline and relevant background information.\n",
    "*   **Pandas:** To efficiently manage and load background data, which is expected to be pre-processed and saved in a pickle file. This background data contains text and pre-calculated embeddings for similarity searches.\n",
    "*   **Pickle:** To load the background data quickly from a `.pkl` file, preserving the data structure and embeddings.\n",
    "*   **LaTeX:** To format the generated book content into a professional, high-quality PDF document.\n",
    "\n",
    "**Here's a high-level overview of what the code does:**\n",
    "\n",
    "1.  **Loads Book Outline:** Reads a JSON file (`book_outline.json`) that defines the structure of your book (sections, titles, goals, and required background for each section).\n",
    "2.  **Loads Background Data:** Loads a pre-processed Pandas DataFrame from a pickle file (`regulations_with_embeddings.pkl`). This DataFrame should contain background text and their corresponding embeddings.\n",
    "3.  **Iterates Through Book Sections:**  Loops through each section defined in the book outline.\n",
    "4.  **Finds Relevant Background Text:** For each section, it uses cosine similarity to find the most relevant background text from the loaded DataFrame based on the \"required background\" description in the outline.\n",
    "5.  **Generates Section Content with OpenAI:**  Uses OpenAI's API to generate the text content for each section, providing the section title, goal, and the most similar background text as context to the language model.\n",
    "6.  **Formats Content in LaTeX:**  Structures the generated text into LaTeX sections, including proper LaTeX preamble and postamble for a complete document.  It also includes basic escaping of LaTeX special characters in titles and preamble.\n",
    "7.  **Saves LaTeX File:**  Saves the complete LaTeX code to a `.tex` file (`generated_book.tex`).\n",
    "8.  **Compiles LaTeX to PDF (Optional):**  Attempts to automatically compile the generated `.tex` file into a PDF document using `pdflatex`.\n",
    "\n",
    "This code provides a framework for automated book generation, and you can customize the outline, background data, prompts, and LaTeX formatting to create your own unique book.  Run the cell to start the book generation process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10ac4ae-b158-4054-bf1e-27d4156d356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openai\n",
    "from typing import Any, Dict\n",
    "import textwrap\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import subprocess\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b3db3-9aac-4382-9391-4cbad8b9c9e0",
   "metadata": {},
   "source": [
    "### A. Parametrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6eecf0-04ec-43b5-99b5-fd3fe14ace25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "JSON_OUTLINE_FILE = \"validation_book_outline.json\"  # Path to your JSON outline file\n",
    "PANDAS_DF_FILE = \"regulations_with_embeddings.pkl\"  # Path to your pickle file with background data\n",
    "TEXT_COLUMN_NAME = \"body_of_the_text\"  # Column with text content\n",
    "EMBEDDING_COLUMN_NAME = \"combined_text_embedding\"  # Column with pre-calculated embeddings\n",
    "OPENAI_MODEL = \"o1-preview\"  # \"gpt-4o\"  # Your preferred OpenAI model\n",
    "LATEX_OUTPUT_FILE = \"generated_book_v1.tex\"  # Name of the output LaTeX file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa23ee-ce15-4582-ae15-a61c4b844bb1",
   "metadata": {},
   "source": [
    "### B. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b3422d-53c6-478e-acc9-691d5a5d9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def load_json_outline(json_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"Loads the book outline from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            outline = json.load(f)\n",
    "        logging.info(f\"Book outline loaded from {json_file}\")\n",
    "        return outline\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading JSON outline: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_dataframe_from_pickle(pickle_filepath: str) -> Any:\n",
    "    \"\"\"Loads a DataFrame from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(pickle_filepath, \"rb\") as f:\n",
    "            loaded_df = pickle.load(f)\n",
    "        logging.info(f\"DataFrame loaded from pickle file: {pickle_filepath}\")\n",
    "        return loaded_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading DataFrame pickle: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_pandas_dataframe(pkl_file: str) -> Any:\n",
    "    \"\"\"\n",
    "    Loads a pandas DataFrame from a pickle file and converts embedding strings to numpy arrays.\n",
    "    Use this if your embeddings are stored as strings.\n",
    "    \"\"\"\n",
    "    df = load_dataframe_from_pickle(pkl_file)\n",
    "    try:\n",
    "        df[EMBEDDING_COLUMN_NAME] = df[EMBEDDING_COLUMN_NAME].apply(\n",
    "            lambda x: np.array(json.loads(x)) if isinstance(x, str) else x\n",
    "        )\n",
    "        logging.info(\"Embeddings converted to numpy arrays (if necessary).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting embeddings: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list:\n",
    "    \"\"\"Generates an embedding for the given text using OpenAI.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        response = openai.embeddings.create(input=[text], model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def find_similar_background_text(df: Any, background_description: str,\n",
    "                                 text_column: str, embedding_column: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the most similar text in the DataFrame to the background description using cosine similarity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description_embedding = get_embedding(background_description)\n",
    "        background_embeddings = np.vstack(df[embedding_column].to_numpy())\n",
    "        similarities = cosine_similarity(np.array([description_embedding]), background_embeddings)\n",
    "        most_similar_index = np.argmax(similarities)\n",
    "        return df[text_column].iloc[most_similar_index]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error finding similar background text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eaf254-dea5-4baf-ba76-e92894f4263f",
   "metadata": {},
   "source": [
    "### C. Content functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b073b2bc-075b-4fab-9f73-90f417ce2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_text(title: str, goal: str, background_text: str,\n",
    "                          content_type: str = \"section\", model: str = OPENAI_MODEL,\n",
    "                          use_langchain: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Generates LaTeX-formatted text for either a section or a subsection.\n",
    "    \n",
    "    Parameters:\n",
    "      - title: The title of the section/subsection.\n",
    "      - goal: The goal of the section/subsection.\n",
    "      - background_text: Background content to guide the writing.\n",
    "      - content_type: Either \"section\" or \"subsection\". This will modify the prompt.\n",
    "      - model: The OpenAI model to use.\n",
    "      - use_langchain: If True, use LangChain's ChatOpenAI; otherwise, use openai.chat.completions.create.\n",
    "    \"\"\"\n",
    "    content_type_lower = content_type.lower()\n",
    "    if content_type_lower not in [\"section\", \"subsection\"]:\n",
    "        raise ValueError(\"content_type must be either 'section' or 'subsection'\")\n",
    "    \n",
    "    content_label = content_type_lower.capitalize()\n",
    "    \n",
    "    header = f\"\"\"\n",
    "        You are a helpful AI assistant specialized in writing technical books about regulatory compliance and model validation in finance.\n",
    "\n",
    "        {content_label} Title: {title}\n",
    "\n",
    "        Goal of this {content_type_lower}: {goal}\n",
    "\n",
    "        Background information to consider when writing this {content_type_lower}:\n",
    "        {background_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    instructions = f\"\"\"\n",
    "        ---\n",
    "        Write the content for the {content_type_lower} above, keeping in mind the goal and background information.\n",
    "        Format the output as LaTeX, suitable for inclusion in a LaTeX document.\n",
    "        Please use standard LaTeX commands for formatting (e.g., \\\\textbf{{important text}}, \\\\textit{{emphasized text}}) and ensure {'correct' if content_type_lower == 'section' else 'proper'} LaTeX syntax.\n",
    "        If you need to include lists, use LaTeX list environments like \\\\begin{{itemize}} ... \\\\end{{itemize}} or \\\\begin{{enumerate}} ... \\\\end{{enumerate}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    if content_type_lower == \"section\":\n",
    "        instructions += \"For mathematical formulas, use inline math mode $...$ or display math mode \\\\begin{{equation}} ... \\\\end{{equation}}.\\n\"\n",
    "    elif content_type_lower == \"subsection\":\n",
    "        instructions += \"Do not use mathematical formulas.\\n\\n\"\n",
    "        instructions += \"----\\n\"\n",
    "        instructions += \"When writing Python code, **format the output as valid Python code enclosed in ```python code blocks.**\\n\"\n",
    "        instructions += \"Include comments to explain the code where necessary.\\n\"\n",
    "        instructions += \"Focus on clarity, correctness, and efficiency of the Python code.\\n\"\n",
    "        instructions += \"Do not include any explanations outside of the code block.\\n\"\n",
    "       \n",
    "    else:\n",
    "        raise ValueError(\"content_type must be either 'section' or 'subsection'\")\n",
    "\n",
    "    prompt = header + instructions\n",
    "    \n",
    "    # Remove any unwanted indentation from the multi-line string.\n",
    "    prompt = textwrap.dedent(prompt)\n",
    "\n",
    "    try:\n",
    "        if use_langchain:\n",
    "            llm = ChatOpenAI(model_name=model, temperature=1.0)\n",
    "            with get_openai_callback() as cb:\n",
    "                response = llm.invoke(prompt)\n",
    "                content = response.content.strip()\n",
    "                logging.info(\"LLM call for content generation completed.\")\n",
    "                logging.info(f\"Generation token usage: {cb.total_tokens} (Prompt: {cb.prompt_tokens}, \"\n",
    "                             f\"Completion: {cb.completion_tokens}, Cost: ${cb.total_cost:.4f})\")\n",
    "            return content\n",
    "        else:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in LaTeX output for technical books.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                # temperature=0.7,\n",
    "                max_completion_tokens=1700\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating text for {content_type} '{title}': {e}\")\n",
    "        return f\"**Error generating content for this {content_type}. Please check logs.**\"\n",
    "\n",
    "\n",
    "def create_latex_section(section_text: str) -> str:\n",
    "    \"\"\"Formats a section with a LaTeX \\\\section header.\"\"\"\n",
    "    return f\"{section_text}\"\n",
    "\n",
    "\n",
    "def create_latex_subsection(subsection_text: str) -> str:\n",
    "    \"\"\"Formats a subsection with a LaTeX \\\\subsection header.\"\"\"\n",
    "    return f\"{subsection_text}\"\n",
    "\n",
    "\n",
    "def create_latex_preamble(title: str, author: str, header_text: str = 'Validation Standards') -> str:\n",
    "    \"\"\"\n",
    "    Creates the LaTeX preamble for the document, including a custom title page.\n",
    "    \"\"\"\n",
    "    preamble = f\"\"\"\n",
    "\\\\documentclass[12pt,a4paper]{{article}}\n",
    "\n",
    "\\\\usepackage[utf8]{{inputenc}}\n",
    "\\\\usepackage[T1]{{fontenc}}\n",
    "\\\\usepackage{{lmodern}}\n",
    "\\\\usepackage[margin=1in]{{geometry}}\n",
    "\\\\usepackage{{setspace}}\n",
    "\\\\usepackage{{titlesec}}\n",
    "\\\\usepackage{{etoolbox}}\n",
    "\\\\usepackage{{fancyhdr}}\n",
    "\\\\usepackage{{graphicx}}\n",
    "\\\\usepackage{{amsmath}}\n",
    "\\\\usepackage{{listings}} % For code listings\n",
    "\\\\lstset{{\n",
    "  basicstyle=\\\\ttfamily\\\\footnotesize,\n",
    "  breaklines=true,\n",
    "  showstringspaces=false\n",
    "}}\n",
    "\n",
    "% Ensure each \\\\section begins on a new page\n",
    "\\\\preto\\\\section{{\\\\clearpage}}\n",
    "\n",
    "% Format for section and subsection titles\n",
    "\\\\titleformat{{\\\\section}}{{\\\\large\\\\bfseries}}{{\\\\thesection}}{{1em}}{{}}\n",
    "\\\\titleformat{{\\\\subsection}}{{\\\\normalsize\\\\bfseries}}{{\\\\thesubsection}}{{1em}}{{}}\n",
    "\n",
    "\\\\setlength{{\\\\parindent}}{{10pt}}\n",
    "\\\\setlength{{\\\\parskip}}{{0.5\\\\baselineskip}}\n",
    "\\\\setlength{{\\\\headheight}}{{14.5pt}}\n",
    "\n",
    "\\\\pagestyle{{fancy}}\n",
    "\\\\fancyhf{{}}\n",
    "\\\\fancyhead[C]{{{header_text}}}\n",
    "\\\\fancyfoot[C]{{\\\\thepage}}\n",
    "\\\\renewcommand{{\\\\headrulewidth}}{{0pt}}\n",
    "\n",
    "\\\\begin{{document}}\n",
    "\\\\pagenumbering{{gobble}}\n",
    "\n",
    "% --- Custom Title Page ---\n",
    "\\\\begin{{titlepage}}\n",
    "    \\\\begin{{center}}\n",
    "        \\\\vspace*{{3cm}}\n",
    "        \n",
    "        {{\\\\Huge \\\\textbf{{{title}}}}}\\\\\\\\[0.8em]\n",
    "        {{\\\\Huge \\\\textbf{{credit risk validation tests}}}}\\\\\\\\[2.0cm]\n",
    "        \n",
    "        {{\\\\Large \\\\textit{{Review and application of key validation models}}}}\\\\\\\\[2.5cm]\n",
    "        \n",
    "        {{\\\\large \\\\textbf{{{author}}}}}\\\\\\\\[0.5cm]\n",
    "        \n",
    "        \\\\vfill\n",
    "        {{\\\\large \\\\today}}\n",
    "    \\\\end{{center}}\n",
    "\\\\end{{titlepage}}\n",
    "\n",
    "\\\\thispagestyle{{empty}}\n",
    "\\\\tableofcontents\n",
    "\\\\pagenumbering{{arabic}}\n",
    "\\\\setcounter{{page}}{{1}}\n",
    "\"\"\"\n",
    "    return preamble\n",
    "\n",
    "def create_latex_postamble() -> str:\n",
    "    \"\"\"Creates the LaTeX postamble for the document.\"\"\"\n",
    "    return \"\\n\\\\end{document}\\n\"\n",
    "\n",
    "\n",
    "def convert_markdown_code_blocks_to_lstlisting(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts Markdown code blocks (```python ... ```) into LaTeX lstlisting environments.\n",
    "    This helps prevent errors from raw backticks in the LaTeX document.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"```python\\s*(.*?)\\s*```\", re.DOTALL)\n",
    "    def repl(match):\n",
    "        code_content = match.group(1)\n",
    "        return \"\\\\begin{lstlisting}[language=Python]\\n\" + code_content + \"\\n\\\\end{lstlisting}\"\n",
    "    return pattern.sub(repl, text)\n",
    "\n",
    "\n",
    "def compile_latex_to_pdf(tex_filename):\n",
    "    \"\"\"\n",
    "    Compiles a .tex file to PDF using pdflatex (requires LaTeX installed).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run pdflatex twice to ensure references are updated if needed\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        subprocess.run([\"pdflatex\", tex_filename], check=True)\n",
    "        print(\"PDF successfully generated.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during LaTeX compilation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130d573-949f-4f7a-9be9-748fd95339c9",
   "metadata": {},
   "source": [
    "### D. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397f5c6d-9b8f-4702-8537-f680905e86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 00:26:38,578 - INFO - Book outline loaded from validation_book_outline.json\n",
      "2025-02-02 00:26:40,418 - INFO - DataFrame loaded from pickle file: regulations_with_embeddings.pkl\n",
      "2025-02-02 00:26:40,423 - INFO - Embeddings converted to numpy arrays (if necessary).\n",
      "2025-02-02 00:26:40,424 - INFO - Generating book content section by section...\n",
      "2025-02-02 00:26:40,424 - INFO - Processing section: General principles\n",
      "2025-02-02 00:26:40,425 - INFO - Finding similar background for section intro...\n",
      "2025-02-02 00:26:41,990 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:26:43,608 - INFO - Generating text with OpenAI for section intro...\n",
      "2025-02-02 00:27:04,771 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:04,783 - INFO - LLM call for content generation completed.\n",
      "2025-02-02 00:27:04,783 - INFO - Generation token usage: 2781 (Prompt: 317, Completion: 2464, Cost: $0.1526)\n",
      "2025-02-02 00:27:04,784 - INFO - Processing subsection: General principles\n",
      "2025-02-02 00:27:04,785 - INFO - Finding similar background for subsection...\n",
      "2025-02-02 00:27:05,579 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:07,221 - INFO - Generating text with LangChain for subsection...\n",
      "2025-02-02 00:27:22,271 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:22,273 - INFO - LLM call for content generation completed.\n",
      "2025-02-02 00:27:22,274 - INFO - Generation token usage: 2174 (Prompt: 364, Completion: 1810, Cost: $0.1141)\n",
      "2025-02-02 00:27:22,274 - INFO - Processing section: The most common validation tests\n",
      "2025-02-02 00:27:22,275 - INFO - Finding similar background for section intro...\n",
      "2025-02-02 00:27:23,085 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:24,661 - INFO - Generating text with OpenAI for section intro...\n",
      "2025-02-02 00:27:45,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:45,528 - INFO - LLM call for content generation completed.\n",
      "2025-02-02 00:27:45,528 - INFO - Generation token usage: 2878 (Prompt: 320, Completion: 2558, Cost: $0.1583)\n",
      "2025-02-02 00:27:45,528 - INFO - Processing subsection: Binomial test\n",
      "2025-02-02 00:27:45,530 - INFO - Finding similar background for subsection...\n",
      "2025-02-02 00:27:46,377 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:27:47,950 - INFO - Generating text with LangChain for subsection...\n",
      "2025-02-02 00:28:12,558 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:28:12,560 - INFO - LLM call for content generation completed.\n",
      "2025-02-02 00:28:12,560 - INFO - Generation token usage: 3174 (Prompt: 349, Completion: 2825, Cost: $0.1747)\n",
      "2025-02-02 00:28:12,560 - INFO - Processing subsection: Chi-Square test (Hosmer-Lemeshow test)\n",
      "2025-02-02 00:28:12,561 - INFO - Finding similar background for subsection...\n",
      "2025-02-02 00:28:13,113 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:28:14,624 - INFO - Generating text with LangChain for subsection...\n",
      "2025-02-02 00:28:43,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-02 00:28:43,810 - INFO - LLM call for content generation completed.\n",
      "2025-02-02 00:28:43,810 - INFO - Generation token usage: 3149 (Prompt: 317, Completion: 2832, Cost: $0.1747)\n",
      "2025-02-02 00:28:43,812 - INFO - Saved LaTeX output to generated_book_v19.tex\n",
      "2025-02-02 00:28:43,812 - INFO - Book generation complete!\n",
      "2025-02-02 00:28:43,813 - INFO - Now you can compile 'generated_book_v19.tex' with LaTeX (e.g., pdflatex).\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the book outline\n",
    "    book_outline = load_json_outline(JSON_OUTLINE_FILE)\n",
    "    \n",
    "    # Load the background DataFrame.\n",
    "    try:\n",
    "        background_df = load_pandas_dataframe(PANDAS_DF_FILE)\n",
    "    except Exception:\n",
    "        logging.info(\"Falling back to direct pickle load for background data.\")\n",
    "        background_df = load_dataframe_from_pickle(PANDAS_DF_FILE)\n",
    "    \n",
    "    # Create the LaTeX preamble\n",
    "    latex_content = create_latex_preamble(\n",
    "        title=\"Review of popular\",\n",
    "        author=\"Collaboration between human and AI\"\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Generating book content section by section...\")\n",
    "\n",
    "    for section_title, section_content in book_outline[\"sections\"].items():\n",
    "        logging.info(f\"Processing section: {section_title}\")\n",
    "        section_goal = section_content.get(\"goal\", \"\")\n",
    "        required_background = section_content.get(\"required_background\", \"\")\n",
    "    \n",
    "        logging.info(\"Finding similar background for section intro...\")\n",
    "        similar_background_text = find_similar_background_text(\n",
    "            background_df, required_background, TEXT_COLUMN_NAME, EMBEDDING_COLUMN_NAME\n",
    "        )\n",
    "    \n",
    "        logging.info(\"Generating text with OpenAI for section intro...\")\n",
    "        generated_section_text = generate_content_text(\n",
    "            title=section_title,\n",
    "            goal=section_goal,\n",
    "            background_text=similar_background_text,\n",
    "            content_type=\"section\",\n",
    "            model=OPENAI_MODEL,\n",
    "            use_langchain=True  # Change to True if you prefer LangChain for section generation\n",
    "        )\n",
    "    \n",
    "        # Convert any Markdown code blocks to lstlisting environments\n",
    "        generated_section_text = convert_markdown_code_blocks_to_lstlisting(generated_section_text)\n",
    "        latex_content += create_latex_section(generated_section_text)\n",
    "    \n",
    "        # Process subsections if any\n",
    "        for subsection_data in section_content.get(\"subsections\", []):\n",
    "            subsection_title = subsection_data.get(\"subsection_title\", \"Untitled Subsection\")\n",
    "            subsection_goal = subsection_data.get(\"goal\", \"\")\n",
    "            sub_required_background = subsection_data.get(\"required_background\", \"\")\n",
    "    \n",
    "            logging.info(f\"Processing subsection: {subsection_title}\")\n",
    "            logging.info(\"Finding similar background for subsection...\")\n",
    "            similar_background_text = find_similar_background_text(\n",
    "                background_df, sub_required_background, TEXT_COLUMN_NAME, EMBEDDING_COLUMN_NAME\n",
    "            )\n",
    "    \n",
    "            logging.info(\"Generating text with LangChain for subsection...\")\n",
    "            generated_subsection_text = generate_content_text(\n",
    "                title=subsection_title,\n",
    "                goal=subsection_goal,\n",
    "                background_text=similar_background_text,\n",
    "                content_type=\"subsection\",\n",
    "                model=OPENAI_MODEL,\n",
    "                use_langchain=True  # Using LangChain for subsections; adjust as desired\n",
    "            )\n",
    "    \n",
    "            # Convert any Markdown code blocks in the subsection\n",
    "            generated_subsection_text = convert_markdown_code_blocks_to_lstlisting(generated_subsection_text)\n",
    "            latex_content += create_latex_subsection(generated_subsection_text)\n",
    "    \n",
    "    latex_content += create_latex_postamble()\n",
    "    \n",
    "    # Save the LaTeX output to a file\n",
    "    try:\n",
    "        with open(LATEX_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(latex_content)\n",
    "        logging.info(f\"Saved LaTeX output to {LATEX_OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving LaTeX file: {e}\")\n",
    "    \n",
    "    logging.info(\"Book generation complete!\")\n",
    "    logging.info(f\"Now you can compile '{LATEX_OUTPUT_FILE}' with LaTeX (e.g., pdflatex).\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ce238-29a2-4cb7-96d1-575f4982853d",
   "metadata": {},
   "source": [
    "### E. Convert to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abdae39-036f-4689-afe4-34dc18da61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during LaTeX compilation: Command '['pdflatex', 'generated_book_v19.tex']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "compile_latex_to_pdf(\"generated_book_v1.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c301cc-6f0a-4ff1-88b7-e8b8cb6f527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdflatex -output-directory=C:/projects/generate_documents/latex_materials generated_book_v19.tex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
