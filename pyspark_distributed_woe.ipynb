{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2bdb0a-dc23-4618-a6cb-0f10901bd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from optbinning import OptimalBinning\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c47b8-1d39-48f9-a2fb-d84acfcb0ad4",
   "metadata": {},
   "source": [
    "### A. Create a dummy PySpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec510abf-350b-48e2-85bb-10e301d8a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"OptBinningExample\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Generate the target variable\n",
    "np.random.seed(42)\n",
    "observations = 1_000\n",
    "target = np.random.randint(0, 2, observations)\n",
    "\n",
    "# Create correlated variables\n",
    "data_pandas = pd.DataFrame({\n",
    "    'var1': (target + 0.2) * np.random.randn(observations),\n",
    "    'var2': (target + 0.3) * np.random.randn(observations),\n",
    "    'var3': (-target + 0.2) * np.random.randn(observations),\n",
    "    'var4': (target + 0.1) * np.random.randn(observations),\n",
    "    'var5': (target + 0.25) * np.random.randn(observations),\n",
    "    'var6': np.random.rand(observations),\n",
    "    'var7': np.random.rand(observations),\n",
    "    'var8': np.random.rand(observations),\n",
    "    'var9': np.random.rand(observations),\n",
    "    'var10': np.random.rand(observations),\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "# Convert to PySpark dataframe\n",
    "data_pyspark = spark.createDataFrame(data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32945-d18d-4253-b97b-2f46216b17c0",
   "metadata": {},
   "source": [
    "### B. Create optimal WoE bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22006d7c-8130-4f2c-9d39-2b7ce31bc60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: var1, IV: 3.807, Splits: [-0.43191871 -0.28221758  0.22032083  0.51510692]\n",
      "Variable: var10, IV: 0.054, Splits: [0.11854626 0.31879368 0.37129214 0.92411289]\n",
      "Variable: var2, IV: 2.298, Splits: [-0.53381741 -0.24581693  0.45663874  0.63207525]\n",
      "Variable: var3, IV: 2.675, Splits: [-0.4505313  -0.24247514  0.1479005   0.36852024]\n",
      "Variable: var4, IV: 3.571, Splits: [-0.16165832 -0.09186092  0.12112483  0.19900268]\n",
      "Variable: var5, IV: 3.132, Splits: [-0.55031744 -0.14298058  0.23498204  0.46967517]\n",
      "Variable: var6, IV: 0.023, Splits: [0.09464763 0.15141959 0.24033074 0.90484789]\n",
      "Variable: var7, IV: 0.035, Splits: [0.12587149 0.17060392 0.32343714 0.59561449]\n",
      "Variable: var8, IV: 0.032, Splits: [0.3757185  0.51269495 0.5759775  0.94276121]\n",
      "Variable: var9, IV: 0.075, Splits: [0.23872217 0.30109173 0.3597669  0.91622782]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def binning_function(var, broadcast_chunk):\n",
    "    # Perform Optimal Binning for a single variable using the chunk data\n",
    "    optb = OptimalBinning(name=var, dtype=\"numerical\", solver=\"cp\", max_n_bins=5)\n",
    "    optb.fit(np.array(broadcast_chunk.value[var]), np.array(broadcast_chunk.value['target']))\n",
    "\n",
    "    # Get Information Value\n",
    "    binning_table = optb.binning_table\n",
    "    binning_table.build()\n",
    "    iv = binning_table.iv\n",
    "    splits = binning_table.splits\n",
    "    \n",
    "    # Return the results as a dictionary\n",
    "    return {\n",
    "        'variable': var,\n",
    "        'iv': iv,\n",
    "        'splits': splits\n",
    "    }\n",
    "\n",
    "def process_in_chunks(data, data_columns, n):\n",
    "    # Split the data_columns into chunks of size n\n",
    "    for i in range(0, len(data_columns), n):\n",
    "        chunk_vars = data_columns[i:i+n]\n",
    "        chunk_data = data[['target'] + list(chunk_vars)]\n",
    "        \n",
    "        # Broadcast the chunk of data\n",
    "        broadcast_chunk = sc.broadcast(chunk_data.to_dict(orient='list'))\n",
    "        \n",
    "        # Run the binning function for each variable in the chunk\n",
    "        chunk_results = sc.parallelize(chunk_vars).map(lambda var: binning_function(var, broadcast_chunk)).collect()\n",
    "        \n",
    "        # Unpersist the broadcast variable to free up memory\n",
    "        broadcast_chunk.unpersist()\n",
    "        \n",
    "        yield chunk_results\n",
    "\n",
    "# Set the maximum number of variables per chunk\n",
    "n = 3\n",
    "\n",
    "# Convert pyspark dataset to pandas\n",
    "data = data_pyspark.toPandas()\n",
    "\n",
    "# Get the list of variables excluding the target\n",
    "variables = data.columns.difference(['target'])\n",
    "\n",
    "# Initialize an empty list to store the final results\n",
    "final_results = []\n",
    "\n",
    "# Process variables in chunks and aggregate the results\n",
    "for chunk_result in process_in_chunks(data, variables, n):\n",
    "    final_results.extend(chunk_result)\n",
    "\n",
    "# Print the results\n",
    "for result in final_results:\n",
    "    print(f\"Variable: {result['variable']}, IV: {result['iv']:0.3f}, Splits: {result['splits']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
