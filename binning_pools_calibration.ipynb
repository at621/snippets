{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae484e6-c52e-42d3-902b-55163b2b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d35bc1-49ab-42bf-9deb-7ce8b3675898",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "##### Maximize IV value of the bins\n",
    "\n",
    "IV = âˆ‘((Good% - Bad%) * ln(Good% / Bad%)), where Good% and Bad% are the proportions of non-default and default observations, respectively, in each bin.\n",
    "\n",
    "##### Constraints:\n",
    "\n",
    "The default rate per bin should be monotonically increasing or decreasing.\n",
    "The bins should be statistically different (determined by the Chi-square test or another suitable hypothesis test with a chosen significance level).\n",
    "The minimum number of observations per bin should be met.\n",
    "The minimum and maximum number of bins should be within the specified range.\n",
    "\n",
    "##### Variables:\n",
    "\n",
    "Bins: A set of ordinal categories.\n",
    "Default rate: The proportion of default observations (default_flag = 1) in each bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d8a9b-3a7a-47d2-a2fc-90989abdcf9c",
   "metadata": {},
   "source": [
    "##### Sequential Least Squares Quadratic Programming\n",
    "\n",
    "SLSQP stands for Sequential Least Squares Quadratic Programming. It is an optimization algorithm available in the scipy.optimize module. The SLSQP method is a gradient-based optimization algorithm used for solving nonlinear optimization problems with equality and inequality constraints. It is particularly suitable for solving problems that have a smooth objective function and smooth constraints. The method takes an objective function, an initial guess for the solution, bounds on the variables, and constraints as input. It then tries to find the optimal solution that minimizes the objective function while satisfying the constraints.\n",
    "\n",
    "Here's a brief overview of how the SLSQP algorithm works:\n",
    "\n",
    "- The algorithm starts with an initial guess for the solution and calculates the objective function value, its gradient (first-order derivatives), and the constraint values and their gradients at this initial point.\n",
    "- It then iteratively improves the solution by approximating the objective function and constraints with quadratic functions (hence the name Quadratic Programming) using the calculated gradient information. The algorithm solves a sequence of quadratic subproblems subject to linearized constraints to find a new solution.\n",
    "- The new solution is checked for feasibility (i.e., whether it satisfies the constraints) and optimality (i.e., whether the objective function value has improved). If necessary, the algorithm adjusts the solution using a line search method to satisfy the constraints.\n",
    "- The algorithm proceeds iteratively, updating the solution and recalculating the objective function, gradients, and constraint values until convergence is achieved, or a stopping criterion is met.\n",
    "\n",
    "SLSQP is a good choice for problems with a moderate number of variables and constraints where the objective function and constraints are smooth (i.e., differentiable). However, it may not be the best choice for large-scale, non-smooth, or non-convex problems. In such cases, other optimization algorithms might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a9533-94a4-44c9-bab6-c50b3ead2e68",
   "metadata": {},
   "source": [
    "### A. Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9913a43e-ec8b-46ac-a5f4-cebe59c68825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   explanatory_variable  target_variable  year  economic_cycle\n",
      "0              6.678090                0  2019        0.140130\n",
      "1              8.521948                1  2020        0.914430\n",
      "2              4.038421                1  2018        0.783868\n",
      "3              3.281745                1  2020        0.119363\n",
      "4              8.264844                1  2020        0.568544\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(n):\n",
    "    years = np.random.choice([2016, 2017, 2018, 2019, 2020, 2021], n)\n",
    "    economic_cycle = np.random.rand(n)\n",
    "    explanatory_variable = np.random.rand(n) * 10\n",
    "\n",
    "    # Add noise to the relationship between explanatory_variable and economic_cycle\n",
    "    noise = np.random.normal(0, 0.1, n)\n",
    "\n",
    "    # Define a non-trivial and random relationship between target and explanatory variable\n",
    "    z = (explanatory_variable / 10) * economic_cycle + noise\n",
    "    target_proba = 1 / (1 + np.exp(-z))  # Apply logistic function\n",
    "    target_variable = (target_proba > 0.5).astype(int)\n",
    "\n",
    "    data = {\n",
    "        'explanatory_variable': explanatory_variable,\n",
    "        'target_variable': target_variable,\n",
    "        'year': years,\n",
    "        'economic_cycle': economic_cycle\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Choose the number of observations\n",
    "n = 200_000 \n",
    "\n",
    "# Create dataset\n",
    "df = generate_data(n)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246a74c-874b-416a-a647-a973204b0cf9",
   "metadata": {},
   "source": [
    "### B. Create optimal pooling/calibration option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcac243d-57e9-4e6b-9acd-3c8c4ffa9bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_iv(df, bin_edges):\n",
    "    binned_data = pd.cut(df['explanatory_variable'], bins=bin_edges, include_lowest=True)\n",
    "    bin_summary = df.groupby(binned_data)['target_variable'].agg(['count', 'sum'])\n",
    "    bin_summary['non_target'] = bin_summary['count'] - bin_summary['sum']\n",
    "    \n",
    "    bin_summary['target_dist'] = bin_summary['sum'] / bin_summary['sum'].sum()\n",
    "    bin_summary['non_target_dist'] = bin_summary['non_target'] / bin_summary['non_target'].sum()\n",
    "    \n",
    "    bin_summary['woe'] = np.log(bin_summary['target_dist'] / bin_summary['non_target_dist'])\n",
    "    bin_summary['iv'] = (bin_summary['target_dist'] - bin_summary['non_target_dist']) * bin_summary['woe']\n",
    "    \n",
    "    return bin_summary['iv'].sum()\n",
    "\n",
    "def calculate_event_rates(df, bin_edges):\n",
    "    annual_event_rates = []\n",
    "\n",
    "    for year in df['year'].unique():\n",
    "        df_year = df[df['year'] == year]\n",
    "        bin_indices = np.digitize(df_year['explanatory_variable'], bin_edges)\n",
    "        bin_counts = np.bincount(bin_indices)[1:len(bin_edges)]\n",
    "        event_counts = np.bincount(bin_indices, weights=df_year['target_variable'])[1:len(bin_edges)]\n",
    "        annual_event_rates.append(event_counts / bin_counts)\n",
    "\n",
    "    average_annual_event_rates = np.mean(annual_event_rates, axis=0)\n",
    "    return average_annual_event_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052f5d46-c752-4eb6-bcfb-aa59d2021ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 2, IV: 64.38%\n",
      "Number of edges: 2, IV: 64.38%\n",
      "Number of edges: 2, IV: 64.38%\n",
      "Number of edges: 3, IV: 71.44%\n",
      "Number of edges: 3, IV: 71.44%\n",
      "Number of edges: 3, IV: 71.44%\n",
      "Number of edges: 3, IV: 71.44%\n",
      "Number of edges: 4, IV: 75.07%\n",
      "Number of edges: 4, IV: 75.07%\n",
      "Number of edges: 4, IV: 75.07%\n",
      "Number of edges: 4, IV: 75.07%\n",
      "Number of edges: 4, IV: 75.07%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Number of edges: 5, IV: 76.85%\n",
      "Optimal bin edges: [3.339410015512634e-05, 3.333345296058221, 6.666657198016288, 9.999969099974354]\n",
      "ROC AUC score (non-binned): 0.7423\n",
      "ROC AUC score (binned): 0.7056\n"
     ]
    }
   ],
   "source": [
    "def optimize_iv(df, min_bins=2, max_bins=10, min_bin_size=0.01, monotonic=True):\n",
    "    best_bin_edges = None\n",
    "    best_objective_value = np.inf\n",
    "    \n",
    "    def iv_objective(bin_edges):\n",
    "        extended_edges = [df['explanatory_variable'].min()] + list(bin_edges) + [df['explanatory_variable'].max()]\n",
    "        print(f'Number of edges: {len(bin_edges)}, IV: {calculate_iv(df, extended_edges):.2%}')\n",
    "        return -calculate_iv(df, extended_edges)\n",
    "\n",
    "    def monotonic_constraint(bin_edges):\n",
    "        extended_edges = [df['explanatory_variable'].min()] + list(bin_edges) + [df['explanatory_variable'].max()]\n",
    "        event_rates = calculate_event_rates(df, extended_edges)\n",
    "        \n",
    "        if monotonic:\n",
    "            if np.all(np.diff(event_rates) >= 0) or np.all(np.diff(event_rates) <= 0):\n",
    "                return 0\n",
    "            else:\n",
    "                return np.inf\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def bins_statistical_difference(bin_edges, df, significance_level=0.05):\n",
    "        extended_edges = [df['explanatory_variable'].min()] + list(bin_edges) + [df['explanatory_variable'].max()]\n",
    "        df['temp_bin'] = pd.cut(df['explanatory_variable'], bins=extended_edges, labels=False, include_lowest=True)\n",
    "\n",
    "        for i in range(len(extended_edges) - 2):\n",
    "            contingency_table = pd.crosstab(df[df['temp_bin'].isin([i, i + 1])]['temp_bin'], df['target_variable'])\n",
    "            _, p, _, _ = chi2_contingency(contingency_table)\n",
    "            # print(f'p-value: {p}')\n",
    "\n",
    "            if p > significance_level:\n",
    "                return np.inf\n",
    "\n",
    "        return 0\n",
    "\n",
    "    for num_bins in range(min_bins, max_bins + 1):\n",
    "        initial_bin_edges = np.linspace(df['explanatory_variable'].min(), df['explanatory_variable'].max(), num_bins + 1)[1:-1]\n",
    "\n",
    "        constraints = [{'type': 'ineq', 'fun': monotonic_constraint},\n",
    "                       {'type': 'ineq', 'fun': lambda x: bins_statistical_difference(x, df)}\n",
    "                      ]\n",
    "        bounds = [(min_bin_size, None) for _ in range(num_bins - 1)]\n",
    "\n",
    "        result = minimize(iv_objective, initial_bin_edges, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "        if result.success and -result.fun < best_objective_value:\n",
    "            best_objective_value = -result.fun\n",
    "            best_bin_edges = [df['explanatory_variable'].min()] + list(result.x) + [df['explanatory_variable'].max()]\n",
    "\n",
    "    return best_bin_edges\n",
    "\n",
    "optimal_bin_edges = optimize_iv(df, min_bins=3, max_bins=6)\n",
    "print(\"Optimal bin edges:\", optimal_bin_edges)\n",
    "\n",
    "# Create a new column in the dataset that represents the bin assignments\n",
    "df['bin'] = pd.cut(df['explanatory_variable'], bins=optimal_bin_edges, labels=False, include_lowest=True)\n",
    "\n",
    "# Calculate ROC AUC scores\n",
    "roc_auc_non_binned = roc_auc_score(df['target_variable'], df['explanatory_variable'])\n",
    "roc_auc_binned = roc_auc_score(df['target_variable'], df['bin'])\n",
    "\n",
    "print(f\"ROC AUC score (non-binned): {roc_auc_non_binned:.4f}\")\n",
    "print(f\"ROC AUC score (binned): {roc_auc_binned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafc28c5-a5b6-422e-b72e-1fec9f7e4582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxxxxxxxxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mxxxxxxxxxx\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xxxxxxxxxx' is not defined"
     ]
    }
   ],
   "source": [
    "xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6b56a-2549-42cd-9977-57d37e4c77ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c35c2a-bf40-4b85-a76e-71746d6ed6b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "To determine whether adjacent bins are statistically different, you can perform a hypothesis test, such as the Chi-square test for independence. \n",
    "\n",
    "Here's a step-by-step process for doing this:\n",
    "- Create a contingency table for the adjacent bins, showing the frequency distribution of the binary default flag (0 and 1) for each pair of adjacent bins.\n",
    "- Calculate the expected frequency for each cell in the contingency table under the assumption that the bins are independent. To do this, multiply the row total (sum of binary default flags in a bin) by the column total (sum of observations in the two adjacent bins) and divide by the total number of observations.\n",
    "- Calculate the Chi-square statistic. For each cell, find the difference between the observed frequency and the expected frequency, square the result, and divide by the expected frequency. Add up all these values to get the Chi-square statistic.\n",
    "- Determine the degrees of freedom for the Chi-square test. For a 2x2 contingency table (2 rows, one for each binary default flag, and 2 columns, one for each adjacent bin), the degrees of freedom are (number of rows - 1) * (number of columns - 1) = 1.\n",
    "- Determine the critical value and the p-value for the calculated Chi-square statistic using the Chi-square distribution table or a statistical software with the corresponding degrees of freedom.\n",
    "- Set a significance level (commonly 0.05). If the p-value is less than the chosen significance level, reject the null hypothesis that the adjacent bins are independent, which means there is a statistically significant difference between the bins. If the p-value is greater than the chosen significance level, you cannot reject the null hypothesis, and there is no evidence to suggest a statistically significant difference between the bins.\n",
    "\n",
    "Repeat these steps for each pair of adjacent bins to determine whether there is a statistically significant difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d4b7e-8f7f-4e80-8a94-c50e5a118740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'bin': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D', 'E', 'E', 'E'],\n",
    "    'default_flag': [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform Chi-square test for independence on adjacent bins\n",
    "significance_level = 0.05\n",
    "\n",
    "for i in range(len(df['bin'].unique()) - 1):\n",
    "    bin1 = df['bin'].unique()[i]\n",
    "    bin2 = df['bin'].unique()[i + 1]\n",
    "    contingency_table = pd.crosstab(df[df['bin'].isin([bin1, bin2])]['bin'], df['default_flag'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Chi-square test for {bin1} and {bin2}:\")\n",
    "    print(f\"Chi2 = {chi2:.4f}, p-value = {p:.4f}\")\n",
    "    \n",
    "    if p < significance_level:\n",
    "        print(f\"The difference between bins {bin1} and {bin2} is statistically significant.\\n\")\n",
    "    else:\n",
    "        print(f\"There is no evidence to suggest a statistically significant difference between bins {bin1} and {bin2}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffde896-d31b-4b14-b091-d166f9a6e1c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "data = np.array([\n",
    "    [60, 30],\n",
    "    [20, 50],\n",
    "])\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(data)\n",
    "\n",
    "print(chi2, p_value, dof, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962891c-9845-434d-aba9-2b168dec54b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
